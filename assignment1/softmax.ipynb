{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "149d5b7e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-03T05:14:04.553289Z",
     "start_time": "2023-10-03T05:14:04.429452Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/roark/Documents/Assignments/cs231n spring2023/assignment1/cs231n/datasets\n",
      "/home/roark/Documents/Assignments/cs231n spring2023/assignment1\n"
     ]
    }
   ],
   "source": [
    "# This mounts your Google Drive to the Colab VM.\n",
    "# from google.colab import drive\n",
    "# drive.mount('/content/drive')\n",
    "\n",
    "# TODO: Enter the foldername in your Drive where you have saved the unzipped\n",
    "# assignment folder, e.g. 'cs231n/assignments/assignment1/'\n",
    "FOLDERNAME = '/home/roark/Documents/Assignments/cs231n\\ spring2023/assignment1'\n",
    "assert FOLDERNAME is not None, \"[!] Enter the foldername.\"\n",
    "\n",
    "# Now that we've mounted your Drive, this ensures that\n",
    "# the Python interpreter of the Colab VM can load\n",
    "# python files from within it.\n",
    "import sys\n",
    "# sys.path.append('/content/drive/My Drive/{}'.format(FOLDERNAME))\n",
    "sys.path.append(FOLDERNAME)\n",
    "\n",
    "# This downloads the CIFAR-10 dataset to your Drive\n",
    "# if it doesn't already exist.\n",
    "# %cd /content/drive/My\\ Drive/$FOLDERNAME/cs231n/datasets/\n",
    "# !bash get_datasets.sh\n",
    "# %cd /content/drive/My\\ Drive/$FOLDERNAME\n",
    "%cd $FOLDERNAME/cs231n/datasets/\n",
    "!bash get_datasets.sh\n",
    "%cd $FOLDERNAME"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fdfc27e",
   "metadata": {
    "tags": [
     "pdf-title"
    ]
   },
   "source": [
    "# Softmax exercise\n",
    "\n",
    "*Complete and hand in this completed worksheet (including its outputs and any supporting code outside of the worksheet) with your assignment submission. For more details see the [assignments page](http://vision.stanford.edu/teaching/cs231n/assignments.html) on the course website.*\n",
    "\n",
    "This exercise is analogous to the SVM exercise. You will:\n",
    "\n",
    "- implement a fully-vectorized **loss function** for the Softmax classifier\n",
    "- implement the fully-vectorized expression for its **analytic gradient**\n",
    "- **check your implementation** with numerical gradient\n",
    "- use a validation set to **tune the learning rate and regularization** strength\n",
    "- **optimize** the loss function with **SGD**\n",
    "- **visualize** the final learned weights\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ca6cf2d4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-03T05:14:04.971282Z",
     "start_time": "2023-10-03T05:14:04.555538Z"
    },
    "tags": [
     "pdf-ignore"
    ]
   },
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "from cs231n.data_utils import load_CIFAR10\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = (8.0, 6.0) # set default size of plots\n",
    "plt.rcParams['image.interpolation'] = 'nearest'\n",
    "plt.rcParams['image.cmap'] = 'gray'\n",
    "\n",
    "# for auto-reloading extenrnal modules\n",
    "# see http://stackoverflow.com/questions/1907993/autoreload-of-modules-in-ipython\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9e9bbbbe",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-03T05:14:06.690623Z",
     "start_time": "2023-10-03T05:14:04.973031Z"
    },
    "tags": [
     "pdf-ignore"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train data shape:  (49000, 3073)\n",
      "Train labels shape:  (49000,)\n",
      "Validation data shape:  (1000, 3073)\n",
      "Validation labels shape:  (1000,)\n",
      "Test data shape:  (1000, 3073)\n",
      "Test labels shape:  (1000,)\n",
      "dev data shape:  (500, 3073)\n",
      "dev labels shape:  (500,)\n"
     ]
    }
   ],
   "source": [
    "def get_CIFAR10_data(num_training=49000, num_validation=1000, num_test=1000, num_dev=500):\n",
    "    \"\"\"\n",
    "    Load the CIFAR-10 dataset from disk and perform preprocessing to prepare\n",
    "    it for the linear classifier. These are the same steps as we used for the\n",
    "    SVM, but condensed to a single function.  \n",
    "    \"\"\"\n",
    "    # Load the raw CIFAR-10 data\n",
    "    cifar10_dir = 'cs231n/datasets/cifar-10-batches-py'\n",
    "    \n",
    "    # Cleaning up variables to prevent loading data multiple times (which may cause memory issue)\n",
    "    try:\n",
    "       del X_train, y_train\n",
    "       del X_test, y_test\n",
    "       print('Clear previously loaded data.')\n",
    "    except:\n",
    "       pass\n",
    "\n",
    "    X_train, y_train, X_test, y_test = load_CIFAR10(cifar10_dir)\n",
    "    \n",
    "    # subsample the data\n",
    "    mask = list(range(num_training, num_training + num_validation))\n",
    "    X_val = X_train[mask]\n",
    "    y_val = y_train[mask]\n",
    "    mask = list(range(num_training))\n",
    "    X_train = X_train[mask]\n",
    "    y_train = y_train[mask]\n",
    "    mask = list(range(num_test))\n",
    "    X_test = X_test[mask]\n",
    "    y_test = y_test[mask]\n",
    "    mask = np.random.choice(num_training, num_dev, replace=False)\n",
    "    X_dev = X_train[mask]\n",
    "    y_dev = y_train[mask]\n",
    "    \n",
    "    # Preprocessing: reshape the image data into rows\n",
    "    X_train = np.reshape(X_train, (X_train.shape[0], -1))\n",
    "    X_val = np.reshape(X_val, (X_val.shape[0], -1))\n",
    "    X_test = np.reshape(X_test, (X_test.shape[0], -1))\n",
    "    X_dev = np.reshape(X_dev, (X_dev.shape[0], -1))\n",
    "    \n",
    "    # Normalize the data: subtract the mean image\n",
    "    mean_image = np.mean(X_train, axis = 0)\n",
    "    X_train -= mean_image\n",
    "    X_val -= mean_image\n",
    "    X_test -= mean_image\n",
    "    X_dev -= mean_image\n",
    "    \n",
    "    # add bias dimension and transform into columns\n",
    "    X_train = np.hstack([X_train, np.ones((X_train.shape[0], 1))])\n",
    "    X_val = np.hstack([X_val, np.ones((X_val.shape[0], 1))])\n",
    "    X_test = np.hstack([X_test, np.ones((X_test.shape[0], 1))])\n",
    "    X_dev = np.hstack([X_dev, np.ones((X_dev.shape[0], 1))])\n",
    "    \n",
    "    return X_train, y_train, X_val, y_val, X_test, y_test, X_dev, y_dev\n",
    "\n",
    "\n",
    "# Invoke the above function to get our data.\n",
    "X_train, y_train, X_val, y_val, X_test, y_test, X_dev, y_dev = get_CIFAR10_data()\n",
    "print('Train data shape: ', X_train.shape)\n",
    "print('Train labels shape: ', y_train.shape)\n",
    "print('Validation data shape: ', X_val.shape)\n",
    "print('Validation labels shape: ', y_val.shape)\n",
    "print('Test data shape: ', X_test.shape)\n",
    "print('Test labels shape: ', y_test.shape)\n",
    "print('dev data shape: ', X_dev.shape)\n",
    "print('dev labels shape: ', y_dev.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4c3fb04",
   "metadata": {},
   "source": [
    "## Softmax Classifier\n",
    "\n",
    "Your code for this section will all be written inside `cs231n/classifiers/softmax.py`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "25f2e5e1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-03T05:14:06.775278Z",
     "start_time": "2023-10-03T05:14:06.693411Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 2.421227\n",
      "sanity check: 2.302585\n"
     ]
    }
   ],
   "source": [
    "# First implement the naive softmax loss function with nested loops.\n",
    "# Open the file cs231n/classifiers/softmax.py and implement the\n",
    "# softmax_loss_naive function.\n",
    "\n",
    "from cs231n.classifiers.softmax import softmax_loss_naive\n",
    "import time\n",
    "\n",
    "# Generate a random softmax weight matrix and use it to compute the loss.\n",
    "W = np.random.randn(3073, 10) * 0.0001\n",
    "loss, grad = softmax_loss_naive(W, X_dev, y_dev, 0.0)\n",
    "\n",
    "# As a rough sanity check, our loss should be something close to -log(0.1).\n",
    "print('loss: %f' % loss)\n",
    "print('sanity check: %f' % (-np.log(0.1)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15ebc638",
   "metadata": {
    "tags": [
     "pdf-inline"
    ]
   },
   "source": [
    "**Inline Question 1**\n",
    "\n",
    "Why do we expect our loss to be close to -log(0.1)? Explain briefly.**\n",
    "\n",
    "$\\color{blue}{\\textit Your Answer:}$\n",
    "$$Loss = -\\frac{1}{N}∑_{i=1}^Nlog\\frac{e^{S_{yi}}}{∑_je^S_j}$$\n",
    "当w初始化为均值为0的极小值时，scores也大约为0附件的很小的值，因此$$e^{s_j}≈1$$\n",
    "$$-\\frac{1}{N}∑_{i=1}^Nlog\\frac{e^{S_{yi}}}{∑_je^S_j}≈-\\frac{1}{N}*N*log\\frac{1}{C}≈-log\\frac{1}{10}$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a8cb3eb1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-03T05:14:09.319698Z",
     "start_time": "2023-10-03T05:14:06.776673Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "numerical: 1.394937 analytic: 1.394937, relative error: 6.166010e-08\n",
      "numerical: 1.371441 analytic: 1.371441, relative error: 1.219285e-08\n",
      "numerical: -1.676833 analytic: -1.676833, relative error: 4.208855e-09\n",
      "numerical: 0.512277 analytic: 0.512277, relative error: 1.368770e-07\n",
      "numerical: -0.005008 analytic: -0.005008, relative error: 1.665264e-06\n",
      "numerical: -0.881773 analytic: -0.881773, relative error: 5.467675e-08\n",
      "numerical: 0.933666 analytic: 0.933665, relative error: 9.209473e-08\n",
      "numerical: 0.709353 analytic: 0.709353, relative error: 1.318490e-08\n",
      "numerical: -1.388948 analytic: -1.388948, relative error: 8.664868e-09\n",
      "numerical: 0.150382 analytic: 0.150382, relative error: 3.607515e-08\n",
      "numerical: -0.896422 analytic: -0.896422, relative error: 3.628580e-08\n",
      "numerical: -1.765670 analytic: -1.765670, relative error: 6.207775e-09\n",
      "numerical: -0.354302 analytic: -0.354302, relative error: 7.265477e-08\n",
      "numerical: -0.494962 analytic: -0.494963, relative error: 1.128896e-07\n",
      "numerical: 0.540032 analytic: 0.540032, relative error: 4.676758e-08\n",
      "numerical: 3.114939 analytic: 3.114938, relative error: 1.408750e-08\n",
      "numerical: 0.823094 analytic: 0.823094, relative error: 1.044652e-07\n",
      "numerical: -0.055317 analytic: -0.055317, relative error: 1.950324e-06\n",
      "numerical: -3.242938 analytic: -3.242939, relative error: 1.348992e-08\n",
      "numerical: 2.525639 analytic: 2.525639, relative error: 2.446054e-08\n"
     ]
    }
   ],
   "source": [
    "# Complete the implementation of softmax_loss_naive and implement a (naive)\n",
    "# version of the gradient that uses nested loops.\n",
    "loss, grad = softmax_loss_naive(W, X_dev, y_dev, 0.0)\n",
    "\n",
    "# As we did for the SVM, use numeric gradient checking as a debugging tool.\n",
    "# The numeric gradient should be close to the analytic gradient.\n",
    "from cs231n.gradient_check import grad_check_sparse\n",
    "f = lambda w: softmax_loss_naive(w, X_dev, y_dev, 0.0)[0]\n",
    "grad_numerical = grad_check_sparse(f, W, grad, 10)\n",
    "\n",
    "# similar to SVM case, do another gradient check with regularization\n",
    "loss, grad = softmax_loss_naive(W, X_dev, y_dev, 5e1)\n",
    "f = lambda w: softmax_loss_naive(w, X_dev, y_dev, 5e1)[0]\n",
    "grad_numerical = grad_check_sparse(f, W, grad, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4a4a81d4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-03T05:14:09.470124Z",
     "start_time": "2023-10-03T05:14:09.321788Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "naive loss: 2.421227e+00 computed in 0.063115s\n",
      "vectorized loss: 2.421227e+00 computed in 0.065270s\n",
      "Loss difference: 0.000000\n",
      "Gradient difference: 0.000000\n"
     ]
    }
   ],
   "source": [
    "# Now that we have a naive implementation of the softmax loss function and its gradient,\n",
    "# implement a vectorized version in softmax_loss_vectorized.\n",
    "# The two versions should compute the same results, but the vectorized version should be\n",
    "# much faster.\n",
    "tic = time.time()\n",
    "loss_naive, grad_naive = softmax_loss_naive(W, X_dev, y_dev, 0.000005)\n",
    "toc = time.time()\n",
    "print('naive loss: %e computed in %fs' % (loss_naive, toc - tic))\n",
    "\n",
    "from cs231n.classifiers.softmax import softmax_loss_vectorized\n",
    "tic = time.time()\n",
    "loss_vectorized, grad_vectorized = softmax_loss_vectorized(W, X_dev, y_dev, 0.000005)\n",
    "toc = time.time()\n",
    "print('vectorized loss: %e computed in %fs' % (loss_vectorized, toc - tic))\n",
    "\n",
    "# As we did for the SVM, we use the Frobenius norm to compare the two versions\n",
    "# of the gradient.\n",
    "grad_difference = np.linalg.norm(grad_naive - grad_vectorized, ord='fro')\n",
    "print('Loss difference: %f' % np.abs(loss_naive - loss_vectorized))\n",
    "print('Gradient difference: %f' % grad_difference)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a3453536",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-03T05:27:26.373204Z",
     "start_time": "2023-10-03T05:14:09.475150Z"
    },
    "tags": [
     "code"
    ],
    "test": "tuning"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 0 / 2000: loss 5.497876\n",
      "iteration 100 / 2000: loss 3.531283\n",
      "iteration 200 / 2000: loss 3.265861\n",
      "iteration 300 / 2000: loss 3.246736\n",
      "iteration 400 / 2000: loss 2.944879\n",
      "iteration 500 / 2000: loss 2.855321\n",
      "iteration 600 / 2000: loss 2.954015\n",
      "iteration 700 / 2000: loss 2.920780\n",
      "iteration 800 / 2000: loss 2.894346\n",
      "iteration 900 / 2000: loss 2.789576\n",
      "iteration 1000 / 2000: loss 2.892353\n",
      "iteration 1100 / 2000: loss 2.465013\n",
      "iteration 1200 / 2000: loss 2.369767\n",
      "iteration 1300 / 2000: loss 2.712411\n",
      "iteration 1400 / 2000: loss 2.414555\n",
      "iteration 1500 / 2000: loss 2.576364\n",
      "iteration 1600 / 2000: loss 2.524175\n",
      "iteration 1700 / 2000: loss 2.325411\n",
      "iteration 1800 / 2000: loss 2.466731\n",
      "iteration 1900 / 2000: loss 2.546545\n",
      "iteration 0 / 2000: loss 12.125510\n",
      "iteration 100 / 2000: loss 9.973650\n",
      "iteration 200 / 2000: loss 9.615809\n",
      "iteration 300 / 2000: loss 9.277883\n",
      "iteration 400 / 2000: loss 8.912798\n",
      "iteration 500 / 2000: loss 8.676704\n",
      "iteration 600 / 2000: loss 8.443298\n",
      "iteration 700 / 2000: loss 7.859856\n",
      "iteration 800 / 2000: loss 7.775313\n",
      "iteration 900 / 2000: loss 7.530211\n",
      "iteration 1000 / 2000: loss 7.506044\n",
      "iteration 1100 / 2000: loss 7.217330\n",
      "iteration 1200 / 2000: loss 7.127118\n",
      "iteration 1300 / 2000: loss 6.679294\n",
      "iteration 1400 / 2000: loss 6.733225\n",
      "iteration 1500 / 2000: loss 6.845774\n",
      "iteration 1600 / 2000: loss 6.495026\n",
      "iteration 1700 / 2000: loss 6.175410\n",
      "iteration 1800 / 2000: loss 6.150203\n",
      "iteration 1900 / 2000: loss 5.866688\n",
      "iteration 0 / 2000: loss 18.933571\n",
      "iteration 100 / 2000: loss 16.515664\n",
      "iteration 200 / 2000: loss 15.112473\n",
      "iteration 300 / 2000: loss 14.191994\n",
      "iteration 400 / 2000: loss 13.419989\n",
      "iteration 500 / 2000: loss 13.121077\n",
      "iteration 600 / 2000: loss 12.133731\n",
      "iteration 700 / 2000: loss 11.581446\n",
      "iteration 800 / 2000: loss 11.112543\n",
      "iteration 900 / 2000: loss 10.515096\n",
      "iteration 1000 / 2000: loss 9.988975\n",
      "iteration 1100 / 2000: loss 9.638121\n",
      "iteration 1200 / 2000: loss 9.278361\n",
      "iteration 1300 / 2000: loss 8.737283\n",
      "iteration 1400 / 2000: loss 8.225915\n",
      "iteration 1500 / 2000: loss 8.094601\n",
      "iteration 1600 / 2000: loss 7.583469\n",
      "iteration 1700 / 2000: loss 7.271822\n",
      "iteration 1800 / 2000: loss 6.771814\n",
      "iteration 1900 / 2000: loss 6.723610\n",
      "iteration 0 / 2000: loss 26.470474\n",
      "iteration 100 / 2000: loss 22.264442\n",
      "iteration 200 / 2000: loss 20.082069\n",
      "iteration 300 / 2000: loss 18.456743\n",
      "iteration 400 / 2000: loss 16.986247\n",
      "iteration 500 / 2000: loss 15.752629\n",
      "iteration 600 / 2000: loss 14.649319\n",
      "iteration 700 / 2000: loss 13.614949\n",
      "iteration 800 / 2000: loss 12.704882\n",
      "iteration 900 / 2000: loss 11.750920\n",
      "iteration 1000 / 2000: loss 11.124119\n",
      "iteration 1100 / 2000: loss 10.234938\n",
      "iteration 1200 / 2000: loss 9.445353\n",
      "iteration 1300 / 2000: loss 8.787274\n",
      "iteration 1400 / 2000: loss 8.374567\n",
      "iteration 1500 / 2000: loss 7.811152\n",
      "iteration 1600 / 2000: loss 7.397570\n",
      "iteration 1700 / 2000: loss 6.896754\n",
      "iteration 1800 / 2000: loss 6.441576\n",
      "iteration 1900 / 2000: loss 6.102292\n",
      "iteration 0 / 2000: loss 32.261037\n",
      "iteration 100 / 2000: loss 27.470165\n",
      "iteration 200 / 2000: loss 24.681151\n",
      "iteration 300 / 2000: loss 22.117680\n",
      "iteration 400 / 2000: loss 19.989438\n",
      "iteration 500 / 2000: loss 17.996910\n",
      "iteration 600 / 2000: loss 16.313772\n",
      "iteration 700 / 2000: loss 14.826861\n",
      "iteration 800 / 2000: loss 13.298932\n",
      "iteration 900 / 2000: loss 12.107955\n",
      "iteration 1000 / 2000: loss 11.040132\n",
      "iteration 1100 / 2000: loss 10.198703\n",
      "iteration 1200 / 2000: loss 9.203580\n",
      "iteration 1300 / 2000: loss 8.323570\n",
      "iteration 1400 / 2000: loss 7.707612\n",
      "iteration 1500 / 2000: loss 7.123826\n",
      "iteration 1600 / 2000: loss 6.572518\n",
      "iteration 1700 / 2000: loss 6.124187\n",
      "iteration 1800 / 2000: loss 5.617528\n",
      "iteration 1900 / 2000: loss 5.144097\n",
      "iteration 0 / 2000: loss 40.061960\n",
      "iteration 100 / 2000: loss 33.418458\n",
      "iteration 200 / 2000: loss 28.996114\n",
      "iteration 300 / 2000: loss 25.341632\n",
      "iteration 400 / 2000: loss 22.190354\n",
      "iteration 500 / 2000: loss 19.696030\n",
      "iteration 600 / 2000: loss 17.127986\n",
      "iteration 700 / 2000: loss 15.461552\n",
      "iteration 800 / 2000: loss 13.566328\n",
      "iteration 900 / 2000: loss 12.154500\n",
      "iteration 1000 / 2000: loss 10.747994\n",
      "iteration 1100 / 2000: loss 9.547519\n",
      "iteration 1200 / 2000: loss 8.596890\n",
      "iteration 1300 / 2000: loss 7.640327\n",
      "iteration 1400 / 2000: loss 6.939936\n",
      "iteration 1500 / 2000: loss 6.214524\n",
      "iteration 1600 / 2000: loss 5.698421\n",
      "iteration 1700 / 2000: loss 5.218086\n",
      "iteration 1800 / 2000: loss 4.877786\n",
      "iteration 1900 / 2000: loss 4.477441\n",
      "iteration 0 / 2000: loss 47.078001\n",
      "iteration 100 / 2000: loss 37.742610\n",
      "iteration 200 / 2000: loss 32.276358\n",
      "iteration 300 / 2000: loss 27.449442\n",
      "iteration 400 / 2000: loss 23.578284\n",
      "iteration 500 / 2000: loss 20.125168\n",
      "iteration 600 / 2000: loss 17.387366\n",
      "iteration 700 / 2000: loss 15.087132\n",
      "iteration 800 / 2000: loss 13.062154\n",
      "iteration 900 / 2000: loss 11.395248\n",
      "iteration 1000 / 2000: loss 9.945733\n",
      "iteration 1100 / 2000: loss 8.806304\n",
      "iteration 1200 / 2000: loss 7.725061\n",
      "iteration 1300 / 2000: loss 6.776621\n",
      "iteration 1400 / 2000: loss 6.114171\n",
      "iteration 1500 / 2000: loss 5.359303\n",
      "iteration 1600 / 2000: loss 4.916018\n",
      "iteration 1700 / 2000: loss 4.394772\n",
      "iteration 1800 / 2000: loss 4.046510\n",
      "iteration 1900 / 2000: loss 3.714553\n",
      "iteration 0 / 2000: loss 54.455823\n",
      "iteration 100 / 2000: loss 42.625670\n",
      "iteration 200 / 2000: loss 35.219465\n",
      "iteration 300 / 2000: loss 29.484582\n",
      "iteration 400 / 2000: loss 24.615058\n",
      "iteration 500 / 2000: loss 20.545496\n",
      "iteration 600 / 2000: loss 17.112741\n",
      "iteration 700 / 2000: loss 14.618045\n",
      "iteration 800 / 2000: loss 12.470302\n",
      "iteration 900 / 2000: loss 10.491563\n",
      "iteration 1000 / 2000: loss 9.073750\n",
      "iteration 1100 / 2000: loss 7.790566\n",
      "iteration 1200 / 2000: loss 6.786705\n",
      "iteration 1300 / 2000: loss 5.939623\n",
      "iteration 1400 / 2000: loss 5.124822\n",
      "iteration 1500 / 2000: loss 4.707972\n",
      "iteration 1600 / 2000: loss 4.131774\n",
      "iteration 1700 / 2000: loss 3.883446\n",
      "iteration 1800 / 2000: loss 3.389874\n",
      "iteration 1900 / 2000: loss 3.178323\n",
      "iteration 0 / 2000: loss 59.692161\n",
      "iteration 100 / 2000: loss 46.985366\n",
      "iteration 200 / 2000: loss 38.037268\n",
      "iteration 300 / 2000: loss 30.838142\n",
      "iteration 400 / 2000: loss 25.159421\n",
      "iteration 500 / 2000: loss 20.686876\n",
      "iteration 600 / 2000: loss 16.940488\n",
      "iteration 700 / 2000: loss 13.985198\n",
      "iteration 800 / 2000: loss 11.520761\n",
      "iteration 900 / 2000: loss 9.717687\n",
      "iteration 1000 / 2000: loss 8.163247\n",
      "iteration 1100 / 2000: loss 6.883238\n",
      "iteration 1200 / 2000: loss 5.995754\n",
      "iteration 1300 / 2000: loss 5.111347\n",
      "iteration 1400 / 2000: loss 4.477072\n",
      "iteration 1500 / 2000: loss 4.017033\n",
      "iteration 1600 / 2000: loss 3.603236\n",
      "iteration 1700 / 2000: loss 3.225054\n",
      "iteration 1800 / 2000: loss 2.979249\n",
      "iteration 1900 / 2000: loss 2.725323\n",
      "iteration 0 / 2000: loss 67.618701\n",
      "iteration 100 / 2000: loss 51.393463\n",
      "iteration 200 / 2000: loss 40.652667\n",
      "iteration 300 / 2000: loss 32.155316\n",
      "iteration 400 / 2000: loss 25.556477\n",
      "iteration 500 / 2000: loss 20.376160\n",
      "iteration 600 / 2000: loss 16.379511\n",
      "iteration 700 / 2000: loss 13.210159\n",
      "iteration 800 / 2000: loss 10.781771\n",
      "iteration 900 / 2000: loss 8.791693\n",
      "iteration 1000 / 2000: loss 7.420116\n",
      "iteration 1100 / 2000: loss 6.065250\n",
      "iteration 1200 / 2000: loss 5.214687\n",
      "iteration 1300 / 2000: loss 4.557749\n",
      "iteration 1400 / 2000: loss 3.960864\n",
      "iteration 1500 / 2000: loss 3.549404\n",
      "iteration 1600 / 2000: loss 3.201366\n",
      "iteration 1700 / 2000: loss 2.828295\n",
      "iteration 1800 / 2000: loss 2.650586\n",
      "iteration 1900 / 2000: loss 2.566951\n",
      "iteration 0 / 2000: loss 6.503918\n",
      "iteration 100 / 2000: loss 3.399235\n",
      "iteration 200 / 2000: loss 3.106898\n",
      "iteration 300 / 2000: loss 2.933400\n",
      "iteration 400 / 2000: loss 2.702987\n",
      "iteration 500 / 2000: loss 2.723642\n",
      "iteration 600 / 2000: loss 2.666078\n",
      "iteration 700 / 2000: loss 2.596442\n",
      "iteration 800 / 2000: loss 2.634063\n",
      "iteration 900 / 2000: loss 2.364625\n",
      "iteration 1000 / 2000: loss 2.690016\n",
      "iteration 1100 / 2000: loss 2.279290\n",
      "iteration 1200 / 2000: loss 2.537910\n",
      "iteration 1300 / 2000: loss 2.233054\n",
      "iteration 1400 / 2000: loss 2.494059\n",
      "iteration 1500 / 2000: loss 2.508836\n",
      "iteration 1600 / 2000: loss 2.224581\n",
      "iteration 1700 / 2000: loss 2.299557\n",
      "iteration 1800 / 2000: loss 2.395389\n",
      "iteration 1900 / 2000: loss 2.238170\n",
      "iteration 0 / 2000: loss 12.331471\n",
      "iteration 100 / 2000: loss 9.885030\n",
      "iteration 200 / 2000: loss 9.301546\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 300 / 2000: loss 8.713120\n",
      "iteration 400 / 2000: loss 8.163628\n",
      "iteration 500 / 2000: loss 7.962188\n",
      "iteration 600 / 2000: loss 7.630918\n",
      "iteration 700 / 2000: loss 7.317844\n",
      "iteration 800 / 2000: loss 7.130055\n",
      "iteration 900 / 2000: loss 6.639087\n",
      "iteration 1000 / 2000: loss 6.467731\n",
      "iteration 1100 / 2000: loss 6.211879\n",
      "iteration 1200 / 2000: loss 5.878151\n",
      "iteration 1300 / 2000: loss 5.632713\n",
      "iteration 1400 / 2000: loss 5.624813\n",
      "iteration 1500 / 2000: loss 5.428967\n",
      "iteration 1600 / 2000: loss 5.199264\n",
      "iteration 1700 / 2000: loss 4.996695\n",
      "iteration 1800 / 2000: loss 4.921045\n",
      "iteration 1900 / 2000: loss 4.706597\n",
      "iteration 0 / 2000: loss 19.374917\n",
      "iteration 100 / 2000: loss 15.591277\n",
      "iteration 200 / 2000: loss 13.843738\n",
      "iteration 300 / 2000: loss 12.733361\n",
      "iteration 400 / 2000: loss 11.684613\n",
      "iteration 500 / 2000: loss 10.813545\n",
      "iteration 600 / 2000: loss 9.880511\n",
      "iteration 700 / 2000: loss 9.148490\n",
      "iteration 800 / 2000: loss 8.564586\n",
      "iteration 900 / 2000: loss 7.937011\n",
      "iteration 1000 / 2000: loss 7.314928\n",
      "iteration 1100 / 2000: loss 6.944530\n",
      "iteration 1200 / 2000: loss 6.554254\n",
      "iteration 1300 / 2000: loss 5.943080\n",
      "iteration 1400 / 2000: loss 5.590893\n",
      "iteration 1500 / 2000: loss 5.207252\n",
      "iteration 1600 / 2000: loss 5.039310\n",
      "iteration 1700 / 2000: loss 4.705924\n",
      "iteration 1800 / 2000: loss 4.320599\n",
      "iteration 1900 / 2000: loss 4.302243\n",
      "iteration 0 / 2000: loss 27.705638\n",
      "iteration 100 / 2000: loss 21.278148\n",
      "iteration 200 / 2000: loss 18.487990\n",
      "iteration 300 / 2000: loss 16.348931\n",
      "iteration 400 / 2000: loss 14.442272\n",
      "iteration 500 / 2000: loss 12.803481\n",
      "iteration 600 / 2000: loss 11.344183\n",
      "iteration 700 / 2000: loss 10.129251\n",
      "iteration 800 / 2000: loss 9.009795\n",
      "iteration 900 / 2000: loss 8.286983\n",
      "iteration 1000 / 2000: loss 7.416792\n",
      "iteration 1100 / 2000: loss 6.658386\n",
      "iteration 1200 / 2000: loss 6.016085\n",
      "iteration 1300 / 2000: loss 5.367264\n",
      "iteration 1400 / 2000: loss 4.938570\n",
      "iteration 1500 / 2000: loss 4.596419\n",
      "iteration 1600 / 2000: loss 4.268485\n",
      "iteration 1700 / 2000: loss 3.845229\n",
      "iteration 1800 / 2000: loss 3.814175\n",
      "iteration 1900 / 2000: loss 3.398778\n",
      "iteration 0 / 2000: loss 32.770649\n",
      "iteration 100 / 2000: loss 25.853517\n",
      "iteration 200 / 2000: loss 21.647048\n",
      "iteration 300 / 2000: loss 18.236357\n",
      "iteration 400 / 2000: loss 15.557781\n",
      "iteration 500 / 2000: loss 13.240794\n",
      "iteration 600 / 2000: loss 11.396968\n",
      "iteration 700 / 2000: loss 9.800023\n",
      "iteration 800 / 2000: loss 8.431566\n",
      "iteration 900 / 2000: loss 7.232969\n",
      "iteration 1000 / 2000: loss 6.503996\n",
      "iteration 1100 / 2000: loss 5.653846\n",
      "iteration 1200 / 2000: loss 5.059224\n",
      "iteration 1300 / 2000: loss 4.464999\n",
      "iteration 1400 / 2000: loss 4.146011\n",
      "iteration 1500 / 2000: loss 3.664985\n",
      "iteration 1600 / 2000: loss 3.380922\n",
      "iteration 1700 / 2000: loss 3.130422\n",
      "iteration 1800 / 2000: loss 2.990682\n",
      "iteration 1900 / 2000: loss 2.742332\n",
      "iteration 0 / 2000: loss 39.390744\n",
      "iteration 100 / 2000: loss 29.900818\n",
      "iteration 200 / 2000: loss 23.975423\n",
      "iteration 300 / 2000: loss 19.504364\n",
      "iteration 400 / 2000: loss 15.899890\n",
      "iteration 500 / 2000: loss 13.316805\n",
      "iteration 600 / 2000: loss 10.936825\n",
      "iteration 700 / 2000: loss 9.217901\n",
      "iteration 800 / 2000: loss 7.630555\n",
      "iteration 900 / 2000: loss 6.522648\n",
      "iteration 1000 / 2000: loss 5.602786\n",
      "iteration 1100 / 2000: loss 4.919028\n",
      "iteration 1200 / 2000: loss 4.181931\n",
      "iteration 1300 / 2000: loss 3.616750\n",
      "iteration 1400 / 2000: loss 3.434698\n",
      "iteration 1500 / 2000: loss 3.097385\n",
      "iteration 1600 / 2000: loss 2.873721\n",
      "iteration 1700 / 2000: loss 2.735817\n",
      "iteration 1800 / 2000: loss 2.553264\n",
      "iteration 1900 / 2000: loss 2.403468\n",
      "iteration 0 / 2000: loss 46.376567\n",
      "iteration 100 / 2000: loss 34.320706\n",
      "iteration 200 / 2000: loss 26.419521\n",
      "iteration 300 / 2000: loss 20.526076\n",
      "iteration 400 / 2000: loss 16.189539\n",
      "iteration 500 / 2000: loss 12.648034\n",
      "iteration 600 / 2000: loss 10.317022\n",
      "iteration 700 / 2000: loss 8.409556\n",
      "iteration 800 / 2000: loss 6.836779\n",
      "iteration 900 / 2000: loss 5.691991\n",
      "iteration 1000 / 2000: loss 4.685378\n",
      "iteration 1100 / 2000: loss 3.993982\n",
      "iteration 1200 / 2000: loss 3.656916\n",
      "iteration 1300 / 2000: loss 3.305097\n",
      "iteration 1400 / 2000: loss 2.710142\n",
      "iteration 1500 / 2000: loss 2.659466\n",
      "iteration 1600 / 2000: loss 2.335422\n",
      "iteration 1700 / 2000: loss 2.341715\n",
      "iteration 1800 / 2000: loss 2.158553\n",
      "iteration 1900 / 2000: loss 2.095531\n",
      "iteration 0 / 2000: loss 54.323897\n",
      "iteration 100 / 2000: loss 37.947454\n",
      "iteration 200 / 2000: loss 28.121784\n",
      "iteration 300 / 2000: loss 20.939602\n",
      "iteration 400 / 2000: loss 15.962905\n",
      "iteration 500 / 2000: loss 12.150514\n",
      "iteration 600 / 2000: loss 9.430376\n",
      "iteration 700 / 2000: loss 7.437555\n",
      "iteration 800 / 2000: loss 5.895858\n",
      "iteration 900 / 2000: loss 4.864107\n",
      "iteration 1000 / 2000: loss 4.059722\n",
      "iteration 1100 / 2000: loss 3.457982\n",
      "iteration 1200 / 2000: loss 3.085974\n",
      "iteration 1300 / 2000: loss 2.604616\n",
      "iteration 1400 / 2000: loss 2.423931\n",
      "iteration 1500 / 2000: loss 2.273436\n",
      "iteration 1600 / 2000: loss 2.303153\n",
      "iteration 1700 / 2000: loss 2.031199\n",
      "iteration 1800 / 2000: loss 2.026368\n",
      "iteration 1900 / 2000: loss 1.973557\n",
      "iteration 0 / 2000: loss 61.315254\n",
      "iteration 100 / 2000: loss 41.336565\n",
      "iteration 200 / 2000: loss 29.407456\n",
      "iteration 300 / 2000: loss 21.206841\n",
      "iteration 400 / 2000: loss 15.363131\n",
      "iteration 500 / 2000: loss 11.403598\n",
      "iteration 600 / 2000: loss 8.583182\n",
      "iteration 700 / 2000: loss 6.529650\n",
      "iteration 800 / 2000: loss 5.228864\n",
      "iteration 900 / 2000: loss 4.205505\n",
      "iteration 1000 / 2000: loss 3.576234\n",
      "iteration 1100 / 2000: loss 3.022290\n",
      "iteration 1200 / 2000: loss 2.718046\n",
      "iteration 1300 / 2000: loss 2.494515\n",
      "iteration 1400 / 2000: loss 2.275800\n",
      "iteration 1500 / 2000: loss 2.068754\n",
      "iteration 1600 / 2000: loss 2.002391\n",
      "iteration 1700 / 2000: loss 1.997831\n",
      "iteration 1800 / 2000: loss 1.972151\n",
      "iteration 1900 / 2000: loss 1.882108\n",
      "iteration 0 / 2000: loss 66.497547\n",
      "iteration 100 / 2000: loss 43.573752\n",
      "iteration 200 / 2000: loss 29.682310\n",
      "iteration 300 / 2000: loss 20.466432\n",
      "iteration 400 / 2000: loss 14.379248\n",
      "iteration 500 / 2000: loss 10.181776\n",
      "iteration 600 / 2000: loss 7.500549\n",
      "iteration 700 / 2000: loss 5.754618\n",
      "iteration 800 / 2000: loss 4.370921\n",
      "iteration 900 / 2000: loss 3.613756\n",
      "iteration 1000 / 2000: loss 3.032220\n",
      "iteration 1100 / 2000: loss 2.684902\n",
      "iteration 1200 / 2000: loss 2.397568\n",
      "iteration 1300 / 2000: loss 2.299932\n",
      "iteration 1400 / 2000: loss 2.160904\n",
      "iteration 1500 / 2000: loss 2.043021\n",
      "iteration 1600 / 2000: loss 1.883434\n",
      "iteration 1700 / 2000: loss 1.865442\n",
      "iteration 1800 / 2000: loss 1.948774\n",
      "iteration 1900 / 2000: loss 1.926660\n",
      "iteration 0 / 2000: loss 5.933129\n",
      "iteration 100 / 2000: loss 3.178731\n",
      "iteration 200 / 2000: loss 2.946655\n",
      "iteration 300 / 2000: loss 2.914929\n",
      "iteration 400 / 2000: loss 2.577742\n",
      "iteration 500 / 2000: loss 2.674202\n",
      "iteration 600 / 2000: loss 2.528976\n",
      "iteration 700 / 2000: loss 2.571157\n",
      "iteration 800 / 2000: loss 2.498337\n",
      "iteration 900 / 2000: loss 2.313808\n",
      "iteration 1000 / 2000: loss 2.477788\n",
      "iteration 1100 / 2000: loss 2.374709\n",
      "iteration 1200 / 2000: loss 2.440106\n",
      "iteration 1300 / 2000: loss 2.338200\n",
      "iteration 1400 / 2000: loss 2.270459\n",
      "iteration 1500 / 2000: loss 2.237908\n",
      "iteration 1600 / 2000: loss 2.287723\n",
      "iteration 1700 / 2000: loss 2.186423\n",
      "iteration 1800 / 2000: loss 2.258616\n",
      "iteration 1900 / 2000: loss 2.463013\n",
      "iteration 0 / 2000: loss 12.044911\n",
      "iteration 100 / 2000: loss 9.445788\n",
      "iteration 200 / 2000: loss 8.570893\n",
      "iteration 300 / 2000: loss 7.985954\n",
      "iteration 400 / 2000: loss 7.503757\n",
      "iteration 500 / 2000: loss 7.199617\n",
      "iteration 600 / 2000: loss 6.696816\n",
      "iteration 700 / 2000: loss 6.452788\n",
      "iteration 800 / 2000: loss 6.013018\n",
      "iteration 900 / 2000: loss 5.797404\n",
      "iteration 1000 / 2000: loss 5.540713\n",
      "iteration 1100 / 2000: loss 5.282272\n",
      "iteration 1200 / 2000: loss 5.064500\n",
      "iteration 1300 / 2000: loss 5.014343\n",
      "iteration 1400 / 2000: loss 4.776898\n",
      "iteration 1500 / 2000: loss 4.364435\n",
      "iteration 1600 / 2000: loss 4.267004\n",
      "iteration 1700 / 2000: loss 4.091530\n",
      "iteration 1800 / 2000: loss 3.944017\n",
      "iteration 1900 / 2000: loss 3.794591\n",
      "iteration 0 / 2000: loss 19.105232\n",
      "iteration 100 / 2000: loss 14.874465\n",
      "iteration 200 / 2000: loss 12.965408\n",
      "iteration 300 / 2000: loss 11.552344\n",
      "iteration 400 / 2000: loss 10.550745\n",
      "iteration 500 / 2000: loss 9.340738\n",
      "iteration 600 / 2000: loss 8.451901\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 700 / 2000: loss 7.577712\n",
      "iteration 800 / 2000: loss 6.857596\n",
      "iteration 900 / 2000: loss 6.284734\n",
      "iteration 1000 / 2000: loss 5.704931\n",
      "iteration 1100 / 2000: loss 5.271553\n",
      "iteration 1200 / 2000: loss 4.773626\n",
      "iteration 1300 / 2000: loss 4.522580\n",
      "iteration 1400 / 2000: loss 4.179722\n",
      "iteration 1500 / 2000: loss 3.969775\n",
      "iteration 1600 / 2000: loss 3.633123\n",
      "iteration 1700 / 2000: loss 3.566904\n",
      "iteration 1800 / 2000: loss 3.276550\n",
      "iteration 1900 / 2000: loss 3.125211\n",
      "iteration 0 / 2000: loss 26.644890\n",
      "iteration 100 / 2000: loss 19.711040\n",
      "iteration 200 / 2000: loss 16.546613\n",
      "iteration 300 / 2000: loss 13.908013\n",
      "iteration 400 / 2000: loss 11.880873\n",
      "iteration 500 / 2000: loss 10.106904\n",
      "iteration 600 / 2000: loss 8.871859\n",
      "iteration 700 / 2000: loss 7.725431\n",
      "iteration 800 / 2000: loss 6.562599\n",
      "iteration 900 / 2000: loss 5.742355\n",
      "iteration 1000 / 2000: loss 5.014983\n",
      "iteration 1100 / 2000: loss 4.508280\n",
      "iteration 1200 / 2000: loss 4.009472\n",
      "iteration 1300 / 2000: loss 3.688222\n",
      "iteration 1400 / 2000: loss 3.241503\n",
      "iteration 1500 / 2000: loss 3.011490\n",
      "iteration 1600 / 2000: loss 2.990131\n",
      "iteration 1700 / 2000: loss 2.615184\n",
      "iteration 1800 / 2000: loss 2.483810\n",
      "iteration 1900 / 2000: loss 2.483149\n",
      "iteration 0 / 2000: loss 33.687191\n",
      "iteration 100 / 2000: loss 23.900678\n",
      "iteration 200 / 2000: loss 18.728300\n",
      "iteration 300 / 2000: loss 15.021227\n",
      "iteration 400 / 2000: loss 12.173338\n",
      "iteration 500 / 2000: loss 9.743271\n",
      "iteration 600 / 2000: loss 8.042390\n",
      "iteration 700 / 2000: loss 6.711606\n",
      "iteration 800 / 2000: loss 5.642164\n",
      "iteration 900 / 2000: loss 4.746676\n",
      "iteration 1000 / 2000: loss 4.195587\n",
      "iteration 1100 / 2000: loss 3.638625\n",
      "iteration 1200 / 2000: loss 3.216344\n",
      "iteration 1300 / 2000: loss 2.839904\n",
      "iteration 1400 / 2000: loss 2.678776\n",
      "iteration 1500 / 2000: loss 2.484814\n",
      "iteration 1600 / 2000: loss 2.332723\n",
      "iteration 1700 / 2000: loss 2.250325\n",
      "iteration 1800 / 2000: loss 2.086450\n",
      "iteration 1900 / 2000: loss 2.094261\n",
      "iteration 0 / 2000: loss 39.185889\n",
      "iteration 100 / 2000: loss 27.911037\n",
      "iteration 200 / 2000: loss 20.625734\n",
      "iteration 300 / 2000: loss 15.616718\n",
      "iteration 400 / 2000: loss 12.036533\n",
      "iteration 500 / 2000: loss 9.190110\n",
      "iteration 600 / 2000: loss 7.238625\n",
      "iteration 700 / 2000: loss 5.873673\n",
      "iteration 800 / 2000: loss 4.727081\n",
      "iteration 900 / 2000: loss 3.930045\n",
      "iteration 1000 / 2000: loss 3.434278\n",
      "iteration 1100 / 2000: loss 3.001928\n",
      "iteration 1200 / 2000: loss 2.645425\n",
      "iteration 1300 / 2000: loss 2.448946\n",
      "iteration 1400 / 2000: loss 2.393279\n",
      "iteration 1500 / 2000: loss 2.254923\n",
      "iteration 1600 / 2000: loss 2.092590\n",
      "iteration 1700 / 2000: loss 1.909496\n",
      "iteration 1800 / 2000: loss 1.960625\n",
      "iteration 1900 / 2000: loss 1.905760\n",
      "iteration 0 / 2000: loss 46.144465\n",
      "iteration 100 / 2000: loss 30.924599\n",
      "iteration 200 / 2000: loss 21.704570\n",
      "iteration 300 / 2000: loss 15.404267\n",
      "iteration 400 / 2000: loss 11.260774\n",
      "iteration 500 / 2000: loss 8.362315\n",
      "iteration 600 / 2000: loss 6.317334\n",
      "iteration 700 / 2000: loss 4.954484\n",
      "iteration 800 / 2000: loss 4.003196\n",
      "iteration 900 / 2000: loss 3.300357\n",
      "iteration 1000 / 2000: loss 2.804565\n",
      "iteration 1100 / 2000: loss 2.548085\n",
      "iteration 1200 / 2000: loss 2.337495\n",
      "iteration 1300 / 2000: loss 2.189368\n",
      "iteration 1400 / 2000: loss 2.135619\n",
      "iteration 1500 / 2000: loss 2.018854\n",
      "iteration 1600 / 2000: loss 1.985842\n",
      "iteration 1700 / 2000: loss 1.904588\n",
      "iteration 1800 / 2000: loss 1.898798\n",
      "iteration 1900 / 2000: loss 1.899170\n",
      "iteration 0 / 2000: loss 52.694485\n",
      "iteration 100 / 2000: loss 33.486565\n",
      "iteration 200 / 2000: loss 22.127386\n",
      "iteration 300 / 2000: loss 15.138227\n",
      "iteration 400 / 2000: loss 10.446128\n",
      "iteration 500 / 2000: loss 7.455402\n",
      "iteration 600 / 2000: loss 5.433696\n",
      "iteration 700 / 2000: loss 4.208124\n",
      "iteration 800 / 2000: loss 3.432747\n",
      "iteration 900 / 2000: loss 2.808802\n",
      "iteration 1000 / 2000: loss 2.554598\n",
      "iteration 1100 / 2000: loss 2.258402\n",
      "iteration 1200 / 2000: loss 2.191622\n",
      "iteration 1300 / 2000: loss 2.042281\n",
      "iteration 1400 / 2000: loss 1.906375\n",
      "iteration 1500 / 2000: loss 1.942108\n",
      "iteration 1600 / 2000: loss 2.033592\n",
      "iteration 1700 / 2000: loss 1.958096\n",
      "iteration 1800 / 2000: loss 1.931092\n",
      "iteration 1900 / 2000: loss 1.959321\n",
      "iteration 0 / 2000: loss 60.282000\n",
      "iteration 100 / 2000: loss 35.997863\n",
      "iteration 200 / 2000: loss 22.650440\n",
      "iteration 300 / 2000: loss 14.565130\n",
      "iteration 400 / 2000: loss 9.677170\n",
      "iteration 500 / 2000: loss 6.759286\n",
      "iteration 600 / 2000: loss 4.763209\n",
      "iteration 700 / 2000: loss 3.694254\n",
      "iteration 800 / 2000: loss 3.010668\n",
      "iteration 900 / 2000: loss 2.633361\n",
      "iteration 1000 / 2000: loss 2.333858\n",
      "iteration 1100 / 2000: loss 2.112117\n",
      "iteration 1200 / 2000: loss 2.043654\n",
      "iteration 1300 / 2000: loss 2.018472\n",
      "iteration 1400 / 2000: loss 1.926516\n",
      "iteration 1500 / 2000: loss 2.010466\n",
      "iteration 1600 / 2000: loss 1.824445\n",
      "iteration 1700 / 2000: loss 1.882903\n",
      "iteration 1800 / 2000: loss 1.899699\n",
      "iteration 1900 / 2000: loss 1.889300\n",
      "iteration 0 / 2000: loss 69.620448\n",
      "iteration 100 / 2000: loss 38.803438\n",
      "iteration 200 / 2000: loss 22.963207\n",
      "iteration 300 / 2000: loss 14.116906\n",
      "iteration 400 / 2000: loss 8.875534\n",
      "iteration 500 / 2000: loss 5.997882\n",
      "iteration 600 / 2000: loss 4.169769\n",
      "iteration 700 / 2000: loss 3.092172\n",
      "iteration 800 / 2000: loss 2.589577\n",
      "iteration 900 / 2000: loss 2.288758\n",
      "iteration 1000 / 2000: loss 2.234600\n",
      "iteration 1100 / 2000: loss 1.985812\n",
      "iteration 1200 / 2000: loss 1.905893\n",
      "iteration 1300 / 2000: loss 1.953535\n",
      "iteration 1400 / 2000: loss 1.899510\n",
      "iteration 1500 / 2000: loss 1.905335\n",
      "iteration 1600 / 2000: loss 1.947862\n",
      "iteration 1700 / 2000: loss 1.952230\n",
      "iteration 1800 / 2000: loss 1.879714\n",
      "iteration 1900 / 2000: loss 1.893723\n",
      "iteration 0 / 2000: loss 6.297792\n",
      "iteration 100 / 2000: loss 3.162111\n",
      "iteration 200 / 2000: loss 3.037908\n",
      "iteration 300 / 2000: loss 2.601043\n",
      "iteration 400 / 2000: loss 2.614709\n",
      "iteration 500 / 2000: loss 2.789734\n",
      "iteration 600 / 2000: loss 2.518920\n",
      "iteration 700 / 2000: loss 2.398021\n",
      "iteration 800 / 2000: loss 2.468750\n",
      "iteration 900 / 2000: loss 2.306421\n",
      "iteration 1000 / 2000: loss 2.202264\n",
      "iteration 1100 / 2000: loss 2.324638\n",
      "iteration 1200 / 2000: loss 2.292056\n",
      "iteration 1300 / 2000: loss 2.337148\n",
      "iteration 1400 / 2000: loss 2.254740\n",
      "iteration 1500 / 2000: loss 2.213635\n",
      "iteration 1600 / 2000: loss 2.111663\n",
      "iteration 1700 / 2000: loss 2.391887\n",
      "iteration 1800 / 2000: loss 2.120086\n",
      "iteration 1900 / 2000: loss 2.383636\n",
      "iteration 0 / 2000: loss 13.148539\n",
      "iteration 100 / 2000: loss 9.180317\n",
      "iteration 200 / 2000: loss 8.419431\n",
      "iteration 300 / 2000: loss 7.741101\n",
      "iteration 400 / 2000: loss 7.035708\n",
      "iteration 500 / 2000: loss 6.638040\n",
      "iteration 600 / 2000: loss 6.276523\n",
      "iteration 700 / 2000: loss 5.915727\n",
      "iteration 800 / 2000: loss 5.422509\n",
      "iteration 900 / 2000: loss 5.110295\n",
      "iteration 1000 / 2000: loss 4.819860\n",
      "iteration 1100 / 2000: loss 4.715277\n",
      "iteration 1200 / 2000: loss 4.440339\n",
      "iteration 1300 / 2000: loss 4.136519\n",
      "iteration 1400 / 2000: loss 4.047909\n",
      "iteration 1500 / 2000: loss 3.749591\n",
      "iteration 1600 / 2000: loss 3.589102\n",
      "iteration 1700 / 2000: loss 3.353117\n",
      "iteration 1800 / 2000: loss 3.331887\n",
      "iteration 1900 / 2000: loss 3.284956\n",
      "iteration 0 / 2000: loss 20.191783\n",
      "iteration 100 / 2000: loss 14.332017\n",
      "iteration 200 / 2000: loss 12.014627\n",
      "iteration 300 / 2000: loss 10.397003\n",
      "iteration 400 / 2000: loss 9.221514\n",
      "iteration 500 / 2000: loss 8.030889\n",
      "iteration 600 / 2000: loss 7.074405\n",
      "iteration 700 / 2000: loss 6.220506\n",
      "iteration 800 / 2000: loss 5.762847\n",
      "iteration 900 / 2000: loss 4.988766\n",
      "iteration 1000 / 2000: loss 4.510397\n",
      "iteration 1100 / 2000: loss 4.181906\n",
      "iteration 1200 / 2000: loss 3.701545\n",
      "iteration 1300 / 2000: loss 3.480778\n",
      "iteration 1400 / 2000: loss 3.211778\n",
      "iteration 1500 / 2000: loss 2.998391\n",
      "iteration 1600 / 2000: loss 2.825181\n",
      "iteration 1700 / 2000: loss 2.729443\n",
      "iteration 1800 / 2000: loss 2.472631\n",
      "iteration 1900 / 2000: loss 2.434068\n",
      "iteration 0 / 2000: loss 28.829859\n",
      "iteration 100 / 2000: loss 19.028667\n",
      "iteration 200 / 2000: loss 15.341031\n",
      "iteration 300 / 2000: loss 12.250885\n",
      "iteration 400 / 2000: loss 10.069683\n",
      "iteration 500 / 2000: loss 8.207109\n",
      "iteration 600 / 2000: loss 6.829202\n",
      "iteration 700 / 2000: loss 5.708164\n",
      "iteration 800 / 2000: loss 4.882236\n",
      "iteration 900 / 2000: loss 4.311750\n",
      "iteration 1000 / 2000: loss 3.690922\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 1100 / 2000: loss 3.185232\n",
      "iteration 1200 / 2000: loss 3.024622\n",
      "iteration 1300 / 2000: loss 2.783022\n",
      "iteration 1400 / 2000: loss 2.539938\n",
      "iteration 1500 / 2000: loss 2.315261\n",
      "iteration 1600 / 2000: loss 2.319602\n",
      "iteration 1700 / 2000: loss 2.123912\n",
      "iteration 1800 / 2000: loss 2.129748\n",
      "iteration 1900 / 2000: loss 1.987255\n",
      "iteration 0 / 2000: loss 33.888995\n",
      "iteration 100 / 2000: loss 22.372229\n",
      "iteration 200 / 2000: loss 16.778926\n",
      "iteration 300 / 2000: loss 12.797692\n",
      "iteration 400 / 2000: loss 9.516067\n",
      "iteration 500 / 2000: loss 7.581520\n",
      "iteration 600 / 2000: loss 6.000653\n",
      "iteration 700 / 2000: loss 4.813806\n",
      "iteration 800 / 2000: loss 4.022879\n",
      "iteration 900 / 2000: loss 3.514332\n",
      "iteration 1000 / 2000: loss 2.994215\n",
      "iteration 1100 / 2000: loss 2.692504\n",
      "iteration 1200 / 2000: loss 2.451137\n",
      "iteration 1300 / 2000: loss 2.161931\n",
      "iteration 1400 / 2000: loss 2.132244\n",
      "iteration 1500 / 2000: loss 2.029609\n",
      "iteration 1600 / 2000: loss 2.023033\n",
      "iteration 1700 / 2000: loss 1.916260\n",
      "iteration 1800 / 2000: loss 1.987797\n",
      "iteration 1900 / 2000: loss 1.900639\n",
      "iteration 0 / 2000: loss 39.515259\n",
      "iteration 100 / 2000: loss 25.355824\n",
      "iteration 200 / 2000: loss 17.798242\n",
      "iteration 300 / 2000: loss 12.401149\n",
      "iteration 400 / 2000: loss 9.083766\n",
      "iteration 500 / 2000: loss 6.707949\n",
      "iteration 600 / 2000: loss 5.050457\n",
      "iteration 700 / 2000: loss 4.102423\n",
      "iteration 800 / 2000: loss 3.306532\n",
      "iteration 900 / 2000: loss 2.800318\n",
      "iteration 1000 / 2000: loss 2.499205\n",
      "iteration 1100 / 2000: loss 2.305394\n",
      "iteration 1200 / 2000: loss 2.136627\n",
      "iteration 1300 / 2000: loss 2.004608\n",
      "iteration 1400 / 2000: loss 1.955901\n",
      "iteration 1500 / 2000: loss 1.894422\n",
      "iteration 1600 / 2000: loss 1.971747\n",
      "iteration 1700 / 2000: loss 1.920772\n",
      "iteration 1800 / 2000: loss 1.910075\n",
      "iteration 1900 / 2000: loss 1.800794\n",
      "iteration 0 / 2000: loss 46.317227\n",
      "iteration 100 / 2000: loss 27.777222\n",
      "iteration 200 / 2000: loss 17.832638\n",
      "iteration 300 / 2000: loss 11.749490\n",
      "iteration 400 / 2000: loss 7.991159\n",
      "iteration 500 / 2000: loss 5.685528\n",
      "iteration 600 / 2000: loss 4.403849\n",
      "iteration 700 / 2000: loss 3.405556\n",
      "iteration 800 / 2000: loss 2.828777\n",
      "iteration 900 / 2000: loss 2.304458\n",
      "iteration 1000 / 2000: loss 2.172368\n",
      "iteration 1100 / 2000: loss 2.043041\n",
      "iteration 1200 / 2000: loss 1.973487\n",
      "iteration 1300 / 2000: loss 1.859786\n",
      "iteration 1400 / 2000: loss 1.915553\n",
      "iteration 1500 / 2000: loss 1.904804\n",
      "iteration 1600 / 2000: loss 1.963562\n",
      "iteration 1700 / 2000: loss 1.789493\n",
      "iteration 1800 / 2000: loss 1.814605\n",
      "iteration 1900 / 2000: loss 1.885090\n",
      "iteration 0 / 2000: loss 52.724922\n",
      "iteration 100 / 2000: loss 29.470199\n",
      "iteration 200 / 2000: loss 17.785771\n",
      "iteration 300 / 2000: loss 11.115047\n",
      "iteration 400 / 2000: loss 7.080889\n",
      "iteration 500 / 2000: loss 4.997256\n",
      "iteration 600 / 2000: loss 3.543147\n",
      "iteration 700 / 2000: loss 2.810393\n",
      "iteration 800 / 2000: loss 2.399424\n",
      "iteration 900 / 2000: loss 2.105542\n",
      "iteration 1000 / 2000: loss 2.083762\n",
      "iteration 1100 / 2000: loss 1.913791\n",
      "iteration 1200 / 2000: loss 1.885129\n",
      "iteration 1300 / 2000: loss 1.871275\n",
      "iteration 1400 / 2000: loss 1.914492\n",
      "iteration 1500 / 2000: loss 1.855138\n",
      "iteration 1600 / 2000: loss 1.857821\n",
      "iteration 1700 / 2000: loss 1.841535\n",
      "iteration 1800 / 2000: loss 1.854801\n",
      "iteration 1900 / 2000: loss 1.801649\n",
      "iteration 0 / 2000: loss 60.718660\n",
      "iteration 100 / 2000: loss 31.663079\n",
      "iteration 200 / 2000: loss 17.651358\n",
      "iteration 300 / 2000: loss 10.249436\n",
      "iteration 400 / 2000: loss 6.345457\n",
      "iteration 500 / 2000: loss 4.293911\n",
      "iteration 600 / 2000: loss 3.176225\n",
      "iteration 700 / 2000: loss 2.621644\n",
      "iteration 800 / 2000: loss 2.239743\n",
      "iteration 900 / 2000: loss 2.066831\n",
      "iteration 1000 / 2000: loss 1.968213\n",
      "iteration 1100 / 2000: loss 1.909688\n",
      "iteration 1200 / 2000: loss 1.914712\n",
      "iteration 1300 / 2000: loss 1.988715\n",
      "iteration 1400 / 2000: loss 1.877466\n",
      "iteration 1500 / 2000: loss 1.936897\n",
      "iteration 1600 / 2000: loss 1.947970\n",
      "iteration 1700 / 2000: loss 1.926701\n",
      "iteration 1800 / 2000: loss 1.903967\n",
      "iteration 1900 / 2000: loss 1.861865\n",
      "iteration 0 / 2000: loss 68.333217\n",
      "iteration 100 / 2000: loss 33.418761\n",
      "iteration 200 / 2000: loss 17.250780\n",
      "iteration 300 / 2000: loss 9.519132\n",
      "iteration 400 / 2000: loss 5.680551\n",
      "iteration 500 / 2000: loss 3.702929\n",
      "iteration 600 / 2000: loss 2.788612\n",
      "iteration 700 / 2000: loss 2.353451\n",
      "iteration 800 / 2000: loss 2.100117\n",
      "iteration 900 / 2000: loss 1.960286\n",
      "iteration 1000 / 2000: loss 2.008921\n",
      "iteration 1100 / 2000: loss 1.898588\n",
      "iteration 1200 / 2000: loss 1.752248\n",
      "iteration 1300 / 2000: loss 1.929894\n",
      "iteration 1400 / 2000: loss 1.941327\n",
      "iteration 1500 / 2000: loss 1.975533\n",
      "iteration 1600 / 2000: loss 1.898047\n",
      "iteration 1700 / 2000: loss 1.908085\n",
      "iteration 1800 / 2000: loss 1.900636\n",
      "iteration 1900 / 2000: loss 1.875633\n",
      "iteration 0 / 2000: loss 5.597063\n",
      "iteration 100 / 2000: loss 3.157817\n",
      "iteration 200 / 2000: loss 2.613402\n",
      "iteration 300 / 2000: loss 2.625596\n",
      "iteration 400 / 2000: loss 2.644960\n",
      "iteration 500 / 2000: loss 2.352001\n",
      "iteration 600 / 2000: loss 2.398566\n",
      "iteration 700 / 2000: loss 2.324071\n",
      "iteration 800 / 2000: loss 2.222106\n",
      "iteration 900 / 2000: loss 2.428599\n",
      "iteration 1000 / 2000: loss 2.250829\n",
      "iteration 1100 / 2000: loss 2.228036\n",
      "iteration 1200 / 2000: loss 2.358178\n",
      "iteration 1300 / 2000: loss 2.090372\n",
      "iteration 1400 / 2000: loss 2.184804\n",
      "iteration 1500 / 2000: loss 2.062700\n",
      "iteration 1600 / 2000: loss 2.364452\n",
      "iteration 1700 / 2000: loss 1.996998\n",
      "iteration 1800 / 2000: loss 2.097535\n",
      "iteration 1900 / 2000: loss 2.131793\n",
      "iteration 0 / 2000: loss 13.369199\n",
      "iteration 100 / 2000: loss 9.152972\n",
      "iteration 200 / 2000: loss 8.017272\n",
      "iteration 300 / 2000: loss 7.292892\n",
      "iteration 400 / 2000: loss 6.763988\n",
      "iteration 500 / 2000: loss 6.140125\n",
      "iteration 600 / 2000: loss 5.722207\n",
      "iteration 700 / 2000: loss 5.359901\n",
      "iteration 800 / 2000: loss 4.914197\n",
      "iteration 900 / 2000: loss 4.724622\n",
      "iteration 1000 / 2000: loss 4.450189\n",
      "iteration 1100 / 2000: loss 4.146572\n",
      "iteration 1200 / 2000: loss 3.826263\n",
      "iteration 1300 / 2000: loss 3.664300\n",
      "iteration 1400 / 2000: loss 3.485998\n",
      "iteration 1500 / 2000: loss 3.257143\n",
      "iteration 1600 / 2000: loss 3.163513\n",
      "iteration 1700 / 2000: loss 2.961709\n",
      "iteration 1800 / 2000: loss 2.887905\n",
      "iteration 1900 / 2000: loss 2.973968\n",
      "iteration 0 / 2000: loss 18.680619\n",
      "iteration 100 / 2000: loss 13.816191\n",
      "iteration 200 / 2000: loss 11.495507\n",
      "iteration 300 / 2000: loss 9.880996\n",
      "iteration 400 / 2000: loss 8.273076\n",
      "iteration 500 / 2000: loss 7.101745\n",
      "iteration 600 / 2000: loss 5.997335\n",
      "iteration 700 / 2000: loss 5.368079\n",
      "iteration 800 / 2000: loss 4.758383\n",
      "iteration 900 / 2000: loss 4.138467\n",
      "iteration 1000 / 2000: loss 3.874809\n",
      "iteration 1100 / 2000: loss 3.222538\n",
      "iteration 1200 / 2000: loss 3.182997\n",
      "iteration 1300 / 2000: loss 2.807364\n",
      "iteration 1400 / 2000: loss 2.621472\n",
      "iteration 1500 / 2000: loss 2.575449\n",
      "iteration 1600 / 2000: loss 2.277109\n",
      "iteration 1700 / 2000: loss 2.287756\n",
      "iteration 1800 / 2000: loss 2.101503\n",
      "iteration 1900 / 2000: loss 2.089230\n",
      "iteration 0 / 2000: loss 26.010357\n",
      "iteration 100 / 2000: loss 17.590374\n",
      "iteration 200 / 2000: loss 13.859163\n",
      "iteration 300 / 2000: loss 10.561034\n",
      "iteration 400 / 2000: loss 8.492463\n",
      "iteration 500 / 2000: loss 6.682527\n",
      "iteration 600 / 2000: loss 5.444595\n",
      "iteration 700 / 2000: loss 4.531010\n",
      "iteration 800 / 2000: loss 3.804853\n",
      "iteration 900 / 2000: loss 3.276187\n",
      "iteration 1000 / 2000: loss 2.884133\n",
      "iteration 1100 / 2000: loss 2.691496\n",
      "iteration 1200 / 2000: loss 2.477690\n",
      "iteration 1300 / 2000: loss 2.277018\n",
      "iteration 1400 / 2000: loss 2.135199\n",
      "iteration 1500 / 2000: loss 2.019795\n",
      "iteration 1600 / 2000: loss 2.016774\n",
      "iteration 1700 / 2000: loss 1.953934\n",
      "iteration 1800 / 2000: loss 1.924832\n",
      "iteration 1900 / 2000: loss 1.832376\n",
      "iteration 0 / 2000: loss 33.453205\n",
      "iteration 100 / 2000: loss 21.385958\n",
      "iteration 200 / 2000: loss 14.804119\n",
      "iteration 300 / 2000: loss 10.674044\n",
      "iteration 400 / 2000: loss 7.788332\n",
      "iteration 500 / 2000: loss 5.838659\n",
      "iteration 600 / 2000: loss 4.608227\n",
      "iteration 700 / 2000: loss 3.693259\n",
      "iteration 800 / 2000: loss 3.118224\n",
      "iteration 900 / 2000: loss 2.588174\n",
      "iteration 1000 / 2000: loss 2.481321\n",
      "iteration 1100 / 2000: loss 2.263775\n",
      "iteration 1200 / 2000: loss 2.149384\n",
      "iteration 1300 / 2000: loss 2.008765\n",
      "iteration 1400 / 2000: loss 1.963015\n",
      "iteration 1500 / 2000: loss 1.928065\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 1600 / 2000: loss 1.875674\n",
      "iteration 1700 / 2000: loss 1.837173\n",
      "iteration 1800 / 2000: loss 1.891866\n",
      "iteration 1900 / 2000: loss 1.854745\n",
      "iteration 0 / 2000: loss 39.814065\n",
      "iteration 100 / 2000: loss 23.674674\n",
      "iteration 200 / 2000: loss 15.313255\n",
      "iteration 300 / 2000: loss 10.059193\n",
      "iteration 400 / 2000: loss 6.937256\n",
      "iteration 500 / 2000: loss 4.900637\n",
      "iteration 600 / 2000: loss 3.817724\n",
      "iteration 700 / 2000: loss 3.117335\n",
      "iteration 800 / 2000: loss 2.538141\n",
      "iteration 900 / 2000: loss 2.288354\n",
      "iteration 1000 / 2000: loss 2.168023\n",
      "iteration 1100 / 2000: loss 1.995646\n",
      "iteration 1200 / 2000: loss 2.014976\n",
      "iteration 1300 / 2000: loss 1.858652\n",
      "iteration 1400 / 2000: loss 2.025104\n",
      "iteration 1500 / 2000: loss 1.887050\n",
      "iteration 1600 / 2000: loss 1.917187\n",
      "iteration 1700 / 2000: loss 1.945444\n",
      "iteration 1800 / 2000: loss 1.781073\n",
      "iteration 1900 / 2000: loss 1.810064\n",
      "iteration 0 / 2000: loss 46.959509\n",
      "iteration 100 / 2000: loss 25.291790\n",
      "iteration 200 / 2000: loss 15.007068\n",
      "iteration 300 / 2000: loss 9.097352\n",
      "iteration 400 / 2000: loss 6.013914\n",
      "iteration 500 / 2000: loss 4.172914\n",
      "iteration 600 / 2000: loss 3.206395\n",
      "iteration 700 / 2000: loss 2.585863\n",
      "iteration 800 / 2000: loss 2.216514\n",
      "iteration 900 / 2000: loss 2.009896\n",
      "iteration 1000 / 2000: loss 1.984311\n",
      "iteration 1100 / 2000: loss 1.996973\n",
      "iteration 1200 / 2000: loss 1.876500\n",
      "iteration 1300 / 2000: loss 1.913149\n",
      "iteration 1400 / 2000: loss 1.849271\n",
      "iteration 1500 / 2000: loss 1.913608\n",
      "iteration 1600 / 2000: loss 1.823263\n",
      "iteration 1700 / 2000: loss 1.871379\n",
      "iteration 1800 / 2000: loss 1.861879\n",
      "iteration 1900 / 2000: loss 1.808362\n",
      "iteration 0 / 2000: loss 53.670017\n",
      "iteration 100 / 2000: loss 26.696858\n",
      "iteration 200 / 2000: loss 14.548057\n",
      "iteration 300 / 2000: loss 8.328604\n",
      "iteration 400 / 2000: loss 5.223398\n",
      "iteration 500 / 2000: loss 3.565934\n",
      "iteration 600 / 2000: loss 2.685703\n",
      "iteration 700 / 2000: loss 2.233885\n",
      "iteration 800 / 2000: loss 2.121158\n",
      "iteration 900 / 2000: loss 1.969246\n",
      "iteration 1000 / 2000: loss 1.964377\n",
      "iteration 1100 / 2000: loss 1.897490\n",
      "iteration 1200 / 2000: loss 1.915826\n",
      "iteration 1300 / 2000: loss 1.861905\n",
      "iteration 1400 / 2000: loss 1.846631\n",
      "iteration 1500 / 2000: loss 1.818724\n",
      "iteration 1600 / 2000: loss 1.857280\n",
      "iteration 1700 / 2000: loss 1.888127\n",
      "iteration 1800 / 2000: loss 1.850085\n",
      "iteration 1900 / 2000: loss 1.968730\n",
      "iteration 0 / 2000: loss 60.967222\n",
      "iteration 100 / 2000: loss 27.857189\n",
      "iteration 200 / 2000: loss 13.896372\n",
      "iteration 300 / 2000: loss 7.476381\n",
      "iteration 400 / 2000: loss 4.492733\n",
      "iteration 500 / 2000: loss 3.174040\n",
      "iteration 600 / 2000: loss 2.519277\n",
      "iteration 700 / 2000: loss 2.252038\n",
      "iteration 800 / 2000: loss 1.934494\n",
      "iteration 900 / 2000: loss 1.847729\n",
      "iteration 1000 / 2000: loss 1.869100\n",
      "iteration 1100 / 2000: loss 1.938170\n",
      "iteration 1200 / 2000: loss 1.853433\n",
      "iteration 1300 / 2000: loss 1.844887\n",
      "iteration 1400 / 2000: loss 1.897077\n",
      "iteration 1500 / 2000: loss 1.822609\n",
      "iteration 1600 / 2000: loss 1.882997\n",
      "iteration 1700 / 2000: loss 1.835857\n",
      "iteration 1800 / 2000: loss 1.922417\n",
      "iteration 1900 / 2000: loss 1.867470\n",
      "iteration 0 / 2000: loss 67.535093\n",
      "iteration 100 / 2000: loss 28.326925\n",
      "iteration 200 / 2000: loss 13.058517\n",
      "iteration 300 / 2000: loss 6.641503\n",
      "iteration 400 / 2000: loss 3.800767\n",
      "iteration 500 / 2000: loss 2.757127\n",
      "iteration 600 / 2000: loss 2.200330\n",
      "iteration 700 / 2000: loss 2.002398\n",
      "iteration 800 / 2000: loss 1.968796\n",
      "iteration 900 / 2000: loss 1.996794\n",
      "iteration 1000 / 2000: loss 1.810865\n",
      "iteration 1100 / 2000: loss 1.917496\n",
      "iteration 1200 / 2000: loss 1.834878\n",
      "iteration 1300 / 2000: loss 1.763692\n",
      "iteration 1400 / 2000: loss 1.869595\n",
      "iteration 1500 / 2000: loss 1.875516\n",
      "iteration 1600 / 2000: loss 1.926386\n",
      "iteration 1700 / 2000: loss 1.810459\n",
      "iteration 1800 / 2000: loss 1.905300\n",
      "iteration 1900 / 2000: loss 1.952286\n",
      "iteration 0 / 2000: loss 5.962369\n",
      "iteration 100 / 2000: loss 3.111430\n",
      "iteration 200 / 2000: loss 2.819456\n",
      "iteration 300 / 2000: loss 2.643436\n",
      "iteration 400 / 2000: loss 2.494363\n",
      "iteration 500 / 2000: loss 2.500511\n",
      "iteration 600 / 2000: loss 2.249534\n",
      "iteration 700 / 2000: loss 2.249071\n",
      "iteration 800 / 2000: loss 2.478470\n",
      "iteration 900 / 2000: loss 2.154291\n",
      "iteration 1000 / 2000: loss 2.187629\n",
      "iteration 1100 / 2000: loss 2.263979\n",
      "iteration 1200 / 2000: loss 2.190539\n",
      "iteration 1300 / 2000: loss 2.209744\n",
      "iteration 1400 / 2000: loss 2.013366\n",
      "iteration 1500 / 2000: loss 2.123379\n",
      "iteration 1600 / 2000: loss 2.165895\n",
      "iteration 1700 / 2000: loss 2.022708\n",
      "iteration 1800 / 2000: loss 2.185417\n",
      "iteration 1900 / 2000: loss 2.030504\n",
      "iteration 0 / 2000: loss 12.765323\n",
      "iteration 100 / 2000: loss 8.952042\n",
      "iteration 200 / 2000: loss 7.897642\n",
      "iteration 300 / 2000: loss 7.034387\n",
      "iteration 400 / 2000: loss 6.580597\n",
      "iteration 500 / 2000: loss 5.763160\n",
      "iteration 600 / 2000: loss 5.271303\n",
      "iteration 700 / 2000: loss 5.040235\n",
      "iteration 800 / 2000: loss 4.447137\n",
      "iteration 900 / 2000: loss 4.280901\n",
      "iteration 1000 / 2000: loss 4.084482\n",
      "iteration 1100 / 2000: loss 3.686552\n",
      "iteration 1200 / 2000: loss 3.431802\n",
      "iteration 1300 / 2000: loss 3.324502\n",
      "iteration 1400 / 2000: loss 3.066905\n",
      "iteration 1500 / 2000: loss 2.839271\n",
      "iteration 1600 / 2000: loss 2.735201\n",
      "iteration 1700 / 2000: loss 2.703317\n",
      "iteration 1800 / 2000: loss 2.520101\n",
      "iteration 1900 / 2000: loss 2.371971\n",
      "iteration 0 / 2000: loss 19.669275\n",
      "iteration 100 / 2000: loss 13.235167\n",
      "iteration 200 / 2000: loss 10.596977\n",
      "iteration 300 / 2000: loss 8.881977\n",
      "iteration 400 / 2000: loss 7.286915\n",
      "iteration 500 / 2000: loss 6.224553\n",
      "iteration 600 / 2000: loss 5.314090\n",
      "iteration 700 / 2000: loss 4.482548\n",
      "iteration 800 / 2000: loss 4.039456\n",
      "iteration 900 / 2000: loss 3.483612\n",
      "iteration 1000 / 2000: loss 3.176491\n",
      "iteration 1100 / 2000: loss 2.861105\n",
      "iteration 1200 / 2000: loss 2.596440\n",
      "iteration 1300 / 2000: loss 2.500202\n",
      "iteration 1400 / 2000: loss 2.416430\n",
      "iteration 1500 / 2000: loss 2.217497\n",
      "iteration 1600 / 2000: loss 2.184844\n",
      "iteration 1700 / 2000: loss 2.057265\n",
      "iteration 1800 / 2000: loss 1.953236\n",
      "iteration 1900 / 2000: loss 1.977839\n",
      "iteration 0 / 2000: loss 26.676096\n",
      "iteration 100 / 2000: loss 17.061416\n",
      "iteration 200 / 2000: loss 12.414017\n",
      "iteration 300 / 2000: loss 9.279222\n",
      "iteration 400 / 2000: loss 7.093305\n",
      "iteration 500 / 2000: loss 5.552584\n",
      "iteration 600 / 2000: loss 4.405205\n",
      "iteration 700 / 2000: loss 3.681986\n",
      "iteration 800 / 2000: loss 3.182635\n",
      "iteration 900 / 2000: loss 2.820186\n",
      "iteration 1000 / 2000: loss 2.400114\n",
      "iteration 1100 / 2000: loss 2.297454\n",
      "iteration 1200 / 2000: loss 2.090099\n",
      "iteration 1300 / 2000: loss 2.201235\n",
      "iteration 1400 / 2000: loss 1.889701\n",
      "iteration 1500 / 2000: loss 1.982464\n",
      "iteration 1600 / 2000: loss 1.887261\n",
      "iteration 1700 / 2000: loss 1.834306\n",
      "iteration 1800 / 2000: loss 1.885394\n",
      "iteration 1900 / 2000: loss 1.897600\n",
      "iteration 0 / 2000: loss 33.024817\n",
      "iteration 100 / 2000: loss 19.500700\n",
      "iteration 200 / 2000: loss 12.942750\n",
      "iteration 300 / 2000: loss 8.776827\n",
      "iteration 400 / 2000: loss 6.316053\n",
      "iteration 500 / 2000: loss 4.693791\n",
      "iteration 600 / 2000: loss 3.532106\n",
      "iteration 700 / 2000: loss 2.985457\n",
      "iteration 800 / 2000: loss 2.569447\n",
      "iteration 900 / 2000: loss 2.218628\n",
      "iteration 1000 / 2000: loss 2.061203\n",
      "iteration 1100 / 2000: loss 2.023440\n",
      "iteration 1200 / 2000: loss 1.907263\n",
      "iteration 1300 / 2000: loss 1.912754\n",
      "iteration 1400 / 2000: loss 1.915826\n",
      "iteration 1500 / 2000: loss 1.866919\n",
      "iteration 1600 / 2000: loss 1.764927\n",
      "iteration 1700 / 2000: loss 1.863192\n",
      "iteration 1800 / 2000: loss 1.859687\n",
      "iteration 1900 / 2000: loss 1.836014\n",
      "iteration 0 / 2000: loss 39.385949\n",
      "iteration 100 / 2000: loss 21.817150\n",
      "iteration 200 / 2000: loss 13.253156\n",
      "iteration 300 / 2000: loss 8.277815\n",
      "iteration 400 / 2000: loss 5.521539\n",
      "iteration 500 / 2000: loss 3.892341\n",
      "iteration 600 / 2000: loss 3.029106\n",
      "iteration 700 / 2000: loss 2.512353\n",
      "iteration 800 / 2000: loss 2.279242\n",
      "iteration 900 / 2000: loss 2.060884\n",
      "iteration 1000 / 2000: loss 2.042991\n",
      "iteration 1100 / 2000: loss 1.973823\n",
      "iteration 1200 / 2000: loss 1.793185\n",
      "iteration 1300 / 2000: loss 1.845296\n",
      "iteration 1400 / 2000: loss 1.766617\n",
      "iteration 1500 / 2000: loss 1.754838\n",
      "iteration 1600 / 2000: loss 1.867675\n",
      "iteration 1700 / 2000: loss 1.841101\n",
      "iteration 1800 / 2000: loss 1.870599\n",
      "iteration 1900 / 2000: loss 1.773505\n",
      "iteration 0 / 2000: loss 46.844777\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 100 / 2000: loss 23.031535\n",
      "iteration 200 / 2000: loss 12.640446\n",
      "iteration 300 / 2000: loss 7.343618\n",
      "iteration 400 / 2000: loss 4.543650\n",
      "iteration 500 / 2000: loss 3.221867\n",
      "iteration 600 / 2000: loss 2.664590\n",
      "iteration 700 / 2000: loss 2.223941\n",
      "iteration 800 / 2000: loss 2.094692\n",
      "iteration 900 / 2000: loss 1.955958\n",
      "iteration 1000 / 2000: loss 1.932178\n",
      "iteration 1100 / 2000: loss 1.822736\n",
      "iteration 1200 / 2000: loss 1.897326\n",
      "iteration 1300 / 2000: loss 1.873750\n",
      "iteration 1400 / 2000: loss 1.860584\n",
      "iteration 1500 / 2000: loss 1.971841\n",
      "iteration 1600 / 2000: loss 1.937446\n",
      "iteration 1700 / 2000: loss 1.874440\n",
      "iteration 1800 / 2000: loss 1.875196\n",
      "iteration 1900 / 2000: loss 1.732834\n",
      "iteration 0 / 2000: loss 53.627097\n",
      "iteration 100 / 2000: loss 23.999002\n",
      "iteration 200 / 2000: loss 11.977168\n",
      "iteration 300 / 2000: loss 6.362317\n",
      "iteration 400 / 2000: loss 3.817422\n",
      "iteration 500 / 2000: loss 2.889333\n",
      "iteration 600 / 2000: loss 2.341980\n",
      "iteration 700 / 2000: loss 2.032846\n",
      "iteration 800 / 2000: loss 2.023962\n",
      "iteration 900 / 2000: loss 1.950166\n",
      "iteration 1000 / 2000: loss 1.888465\n",
      "iteration 1100 / 2000: loss 1.954577\n",
      "iteration 1200 / 2000: loss 1.896936\n",
      "iteration 1300 / 2000: loss 1.919222\n",
      "iteration 1400 / 2000: loss 1.863079\n",
      "iteration 1500 / 2000: loss 1.760164\n",
      "iteration 1600 / 2000: loss 1.929321\n",
      "iteration 1700 / 2000: loss 1.946603\n",
      "iteration 1800 / 2000: loss 1.860113\n",
      "iteration 1900 / 2000: loss 1.852447\n",
      "iteration 0 / 2000: loss 59.508282\n",
      "iteration 100 / 2000: loss 23.993291\n",
      "iteration 200 / 2000: loss 10.892615\n",
      "iteration 300 / 2000: loss 5.446219\n",
      "iteration 400 / 2000: loss 3.367881\n",
      "iteration 500 / 2000: loss 2.370819\n",
      "iteration 600 / 2000: loss 2.157216\n",
      "iteration 700 / 2000: loss 2.011065\n",
      "iteration 800 / 2000: loss 1.968375\n",
      "iteration 900 / 2000: loss 2.000600\n",
      "iteration 1000 / 2000: loss 1.932211\n",
      "iteration 1100 / 2000: loss 1.789725\n",
      "iteration 1200 / 2000: loss 1.833768\n",
      "iteration 1300 / 2000: loss 1.905168\n",
      "iteration 1400 / 2000: loss 1.899892\n",
      "iteration 1500 / 2000: loss 1.885104\n",
      "iteration 1600 / 2000: loss 1.875574\n",
      "iteration 1700 / 2000: loss 1.862834\n",
      "iteration 1800 / 2000: loss 1.848154\n",
      "iteration 1900 / 2000: loss 1.917466\n",
      "iteration 0 / 2000: loss 67.612925\n",
      "iteration 100 / 2000: loss 24.442314\n",
      "iteration 200 / 2000: loss 10.132616\n",
      "iteration 300 / 2000: loss 4.781867\n",
      "iteration 400 / 2000: loss 2.966295\n",
      "iteration 500 / 2000: loss 2.331749\n",
      "iteration 600 / 2000: loss 2.013080\n",
      "iteration 700 / 2000: loss 1.973685\n",
      "iteration 800 / 2000: loss 1.804864\n",
      "iteration 900 / 2000: loss 1.810354\n",
      "iteration 1000 / 2000: loss 1.915760\n",
      "iteration 1100 / 2000: loss 1.832773\n",
      "iteration 1200 / 2000: loss 1.864578\n",
      "iteration 1300 / 2000: loss 1.860930\n",
      "iteration 1400 / 2000: loss 1.968675\n",
      "iteration 1500 / 2000: loss 1.858853\n",
      "iteration 1600 / 2000: loss 2.075832\n",
      "iteration 1700 / 2000: loss 1.877036\n",
      "iteration 1800 / 2000: loss 1.868449\n",
      "iteration 1900 / 2000: loss 1.852657\n",
      "iteration 0 / 2000: loss 6.449066\n",
      "iteration 100 / 2000: loss 2.908188\n",
      "iteration 200 / 2000: loss 2.677792\n",
      "iteration 300 / 2000: loss 2.509001\n",
      "iteration 400 / 2000: loss 2.696468\n",
      "iteration 500 / 2000: loss 2.403055\n",
      "iteration 600 / 2000: loss 2.373877\n",
      "iteration 700 / 2000: loss 2.169354\n",
      "iteration 800 / 2000: loss 2.213069\n",
      "iteration 900 / 2000: loss 2.142734\n",
      "iteration 1000 / 2000: loss 2.231548\n",
      "iteration 1100 / 2000: loss 2.171586\n",
      "iteration 1200 / 2000: loss 2.200969\n",
      "iteration 1300 / 2000: loss 2.221245\n",
      "iteration 1400 / 2000: loss 2.054425\n",
      "iteration 1500 / 2000: loss 2.227498\n",
      "iteration 1600 / 2000: loss 2.048020\n",
      "iteration 1700 / 2000: loss 2.083934\n",
      "iteration 1800 / 2000: loss 2.064127\n",
      "iteration 1900 / 2000: loss 2.151136\n",
      "iteration 0 / 2000: loss 13.511073\n",
      "iteration 100 / 2000: loss 8.785026\n",
      "iteration 200 / 2000: loss 7.721629\n",
      "iteration 300 / 2000: loss 6.837065\n",
      "iteration 400 / 2000: loss 5.856872\n",
      "iteration 500 / 2000: loss 5.332318\n",
      "iteration 600 / 2000: loss 4.913013\n",
      "iteration 700 / 2000: loss 4.437005\n",
      "iteration 800 / 2000: loss 4.177577\n",
      "iteration 900 / 2000: loss 3.910169\n",
      "iteration 1000 / 2000: loss 3.518698\n",
      "iteration 1100 / 2000: loss 3.340207\n",
      "iteration 1200 / 2000: loss 3.090891\n",
      "iteration 1300 / 2000: loss 2.897205\n",
      "iteration 1400 / 2000: loss 2.709427\n",
      "iteration 1500 / 2000: loss 2.550361\n",
      "iteration 1600 / 2000: loss 2.519666\n",
      "iteration 1700 / 2000: loss 2.419160\n",
      "iteration 1800 / 2000: loss 2.357492\n",
      "iteration 1900 / 2000: loss 2.270665\n",
      "iteration 0 / 2000: loss 19.939824\n",
      "iteration 100 / 2000: loss 12.993454\n",
      "iteration 200 / 2000: loss 10.279562\n",
      "iteration 300 / 2000: loss 8.189113\n",
      "iteration 400 / 2000: loss 6.665766\n",
      "iteration 500 / 2000: loss 5.552290\n",
      "iteration 600 / 2000: loss 4.748182\n",
      "iteration 700 / 2000: loss 3.865358\n",
      "iteration 800 / 2000: loss 3.477588\n",
      "iteration 900 / 2000: loss 3.047722\n",
      "iteration 1000 / 2000: loss 2.785733\n",
      "iteration 1100 / 2000: loss 2.545815\n",
      "iteration 1200 / 2000: loss 2.216082\n",
      "iteration 1300 / 2000: loss 2.129645\n",
      "iteration 1400 / 2000: loss 2.179569\n",
      "iteration 1500 / 2000: loss 2.062602\n",
      "iteration 1600 / 2000: loss 1.910996\n",
      "iteration 1700 / 2000: loss 1.935184\n",
      "iteration 1800 / 2000: loss 1.942876\n",
      "iteration 1900 / 2000: loss 1.802906\n",
      "iteration 0 / 2000: loss 26.309189\n",
      "iteration 100 / 2000: loss 16.323207\n",
      "iteration 200 / 2000: loss 11.459428\n",
      "iteration 300 / 2000: loss 8.231909\n",
      "iteration 400 / 2000: loss 6.149777\n",
      "iteration 500 / 2000: loss 4.788822\n",
      "iteration 600 / 2000: loss 3.800402\n",
      "iteration 700 / 2000: loss 3.080023\n",
      "iteration 800 / 2000: loss 2.720178\n",
      "iteration 900 / 2000: loss 2.433686\n",
      "iteration 1000 / 2000: loss 2.168120\n",
      "iteration 1100 / 2000: loss 2.098382\n",
      "iteration 1200 / 2000: loss 2.021824\n",
      "iteration 1300 / 2000: loss 2.003846\n",
      "iteration 1400 / 2000: loss 1.876365\n",
      "iteration 1500 / 2000: loss 1.808727\n",
      "iteration 1600 / 2000: loss 1.816774\n",
      "iteration 1700 / 2000: loss 1.865656\n",
      "iteration 1800 / 2000: loss 1.862443\n",
      "iteration 1900 / 2000: loss 1.755854\n",
      "iteration 0 / 2000: loss 33.410534\n",
      "iteration 100 / 2000: loss 18.320845\n",
      "iteration 200 / 2000: loss 11.594532\n",
      "iteration 300 / 2000: loss 7.495441\n",
      "iteration 400 / 2000: loss 5.273200\n",
      "iteration 500 / 2000: loss 3.888041\n",
      "iteration 600 / 2000: loss 3.043258\n",
      "iteration 700 / 2000: loss 2.463440\n",
      "iteration 800 / 2000: loss 2.267764\n",
      "iteration 900 / 2000: loss 2.134144\n",
      "iteration 1000 / 2000: loss 2.045366\n",
      "iteration 1100 / 2000: loss 2.009224\n",
      "iteration 1200 / 2000: loss 1.819828\n",
      "iteration 1300 / 2000: loss 1.881462\n",
      "iteration 1400 / 2000: loss 1.815506\n",
      "iteration 1500 / 2000: loss 1.745772\n",
      "iteration 1600 / 2000: loss 1.908004\n",
      "iteration 1700 / 2000: loss 1.792469\n",
      "iteration 1800 / 2000: loss 1.832823\n",
      "iteration 1900 / 2000: loss 1.856634\n",
      "iteration 0 / 2000: loss 40.243745\n",
      "iteration 100 / 2000: loss 20.203321\n",
      "iteration 200 / 2000: loss 11.195850\n",
      "iteration 300 / 2000: loss 6.863687\n",
      "iteration 400 / 2000: loss 4.291919\n",
      "iteration 500 / 2000: loss 3.258532\n",
      "iteration 600 / 2000: loss 2.558341\n",
      "iteration 700 / 2000: loss 2.270783\n",
      "iteration 800 / 2000: loss 2.066956\n",
      "iteration 900 / 2000: loss 1.883887\n",
      "iteration 1000 / 2000: loss 1.932832\n",
      "iteration 1100 / 2000: loss 1.827243\n",
      "iteration 1200 / 2000: loss 1.953413\n",
      "iteration 1300 / 2000: loss 1.777845\n",
      "iteration 1400 / 2000: loss 1.928024\n",
      "iteration 1500 / 2000: loss 1.802777\n",
      "iteration 1600 / 2000: loss 1.842586\n",
      "iteration 1700 / 2000: loss 1.920110\n",
      "iteration 1800 / 2000: loss 1.788988\n",
      "iteration 1900 / 2000: loss 1.886430\n",
      "iteration 0 / 2000: loss 45.841287\n",
      "iteration 100 / 2000: loss 20.977651\n",
      "iteration 200 / 2000: loss 10.460646\n",
      "iteration 300 / 2000: loss 5.722410\n",
      "iteration 400 / 2000: loss 3.676612\n",
      "iteration 500 / 2000: loss 2.698731\n",
      "iteration 600 / 2000: loss 2.324026\n",
      "iteration 700 / 2000: loss 2.227851\n",
      "iteration 800 / 2000: loss 2.018939\n",
      "iteration 900 / 2000: loss 1.891809\n",
      "iteration 1000 / 2000: loss 1.833775\n",
      "iteration 1100 / 2000: loss 1.848389\n",
      "iteration 1200 / 2000: loss 1.966197\n",
      "iteration 1300 / 2000: loss 1.883859\n",
      "iteration 1400 / 2000: loss 1.885364\n",
      "iteration 1500 / 2000: loss 1.851091\n",
      "iteration 1600 / 2000: loss 1.851921\n",
      "iteration 1700 / 2000: loss 1.860676\n",
      "iteration 1800 / 2000: loss 1.796765\n",
      "iteration 1900 / 2000: loss 1.877011\n",
      "iteration 0 / 2000: loss 53.327151\n",
      "iteration 100 / 2000: loss 21.441008\n",
      "iteration 200 / 2000: loss 9.650501\n",
      "iteration 300 / 2000: loss 5.206080\n",
      "iteration 400 / 2000: loss 3.043691\n",
      "iteration 500 / 2000: loss 2.396082\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 600 / 2000: loss 1.956684\n",
      "iteration 700 / 2000: loss 2.063960\n",
      "iteration 800 / 2000: loss 2.007659\n",
      "iteration 900 / 2000: loss 1.925635\n",
      "iteration 1000 / 2000: loss 1.907014\n",
      "iteration 1100 / 2000: loss 1.758966\n",
      "iteration 1200 / 2000: loss 1.800008\n",
      "iteration 1300 / 2000: loss 1.872717\n",
      "iteration 1400 / 2000: loss 1.926109\n",
      "iteration 1500 / 2000: loss 1.795156\n",
      "iteration 1600 / 2000: loss 1.941684\n",
      "iteration 1700 / 2000: loss 1.870749\n",
      "iteration 1800 / 2000: loss 1.836587\n",
      "iteration 1900 / 2000: loss 1.941843\n",
      "iteration 0 / 2000: loss 60.836966\n",
      "iteration 100 / 2000: loss 21.357555\n",
      "iteration 200 / 2000: loss 8.805163\n",
      "iteration 300 / 2000: loss 4.247733\n",
      "iteration 400 / 2000: loss 2.771104\n",
      "iteration 500 / 2000: loss 2.129521\n",
      "iteration 600 / 2000: loss 2.042267\n",
      "iteration 700 / 2000: loss 1.869616\n",
      "iteration 800 / 2000: loss 1.969910\n",
      "iteration 900 / 2000: loss 1.878583\n",
      "iteration 1000 / 2000: loss 1.951942\n",
      "iteration 1100 / 2000: loss 1.911699\n",
      "iteration 1200 / 2000: loss 1.841650\n",
      "iteration 1300 / 2000: loss 1.925679\n",
      "iteration 1400 / 2000: loss 1.821503\n",
      "iteration 1500 / 2000: loss 1.815810\n",
      "iteration 1600 / 2000: loss 1.977492\n",
      "iteration 1700 / 2000: loss 1.926914\n",
      "iteration 1800 / 2000: loss 1.834966\n",
      "iteration 1900 / 2000: loss 1.871588\n",
      "iteration 0 / 2000: loss 66.590374\n",
      "iteration 100 / 2000: loss 21.277959\n",
      "iteration 200 / 2000: loss 7.964299\n",
      "iteration 300 / 2000: loss 3.632555\n",
      "iteration 400 / 2000: loss 2.470411\n",
      "iteration 500 / 2000: loss 2.101743\n",
      "iteration 600 / 2000: loss 1.921566\n",
      "iteration 700 / 2000: loss 1.905077\n",
      "iteration 800 / 2000: loss 1.928953\n",
      "iteration 900 / 2000: loss 1.883042\n",
      "iteration 1000 / 2000: loss 1.884461\n",
      "iteration 1100 / 2000: loss 1.927269\n",
      "iteration 1200 / 2000: loss 1.925060\n",
      "iteration 1300 / 2000: loss 1.801513\n",
      "iteration 1400 / 2000: loss 1.805168\n",
      "iteration 1500 / 2000: loss 1.983219\n",
      "iteration 1600 / 2000: loss 1.905953\n",
      "iteration 1700 / 2000: loss 1.937547\n",
      "iteration 1800 / 2000: loss 1.910795\n",
      "iteration 1900 / 2000: loss 1.793309\n",
      "iteration 0 / 2000: loss 6.552528\n",
      "iteration 100 / 2000: loss 2.704074\n",
      "iteration 200 / 2000: loss 2.750067\n",
      "iteration 300 / 2000: loss 2.401072\n",
      "iteration 400 / 2000: loss 2.293960\n",
      "iteration 500 / 2000: loss 2.289973\n",
      "iteration 600 / 2000: loss 2.156054\n",
      "iteration 700 / 2000: loss 2.413492\n",
      "iteration 800 / 2000: loss 2.074647\n",
      "iteration 900 / 2000: loss 2.074771\n",
      "iteration 1000 / 2000: loss 2.203165\n",
      "iteration 1100 / 2000: loss 2.153397\n",
      "iteration 1200 / 2000: loss 2.176644\n",
      "iteration 1300 / 2000: loss 2.111069\n",
      "iteration 1400 / 2000: loss 2.164656\n",
      "iteration 1500 / 2000: loss 1.948520\n",
      "iteration 1600 / 2000: loss 2.200266\n",
      "iteration 1700 / 2000: loss 2.051374\n",
      "iteration 1800 / 2000: loss 2.194227\n",
      "iteration 1900 / 2000: loss 2.117252\n",
      "iteration 0 / 2000: loss 13.065469\n",
      "iteration 100 / 2000: loss 8.419212\n",
      "iteration 200 / 2000: loss 7.381794\n",
      "iteration 300 / 2000: loss 6.548292\n",
      "iteration 400 / 2000: loss 5.689758\n",
      "iteration 500 / 2000: loss 5.047187\n",
      "iteration 600 / 2000: loss 4.518990\n",
      "iteration 700 / 2000: loss 4.136246\n",
      "iteration 800 / 2000: loss 3.839752\n",
      "iteration 900 / 2000: loss 3.465770\n",
      "iteration 1000 / 2000: loss 3.312266\n",
      "iteration 1100 / 2000: loss 3.010118\n",
      "iteration 1200 / 2000: loss 2.816771\n",
      "iteration 1300 / 2000: loss 2.691824\n",
      "iteration 1400 / 2000: loss 2.689969\n",
      "iteration 1500 / 2000: loss 2.443324\n",
      "iteration 1600 / 2000: loss 2.313804\n",
      "iteration 1700 / 2000: loss 2.320311\n",
      "iteration 1800 / 2000: loss 2.178113\n",
      "iteration 1900 / 2000: loss 2.172791\n",
      "iteration 0 / 2000: loss 19.445135\n",
      "iteration 100 / 2000: loss 12.713739\n",
      "iteration 200 / 2000: loss 9.661778\n",
      "iteration 300 / 2000: loss 7.504027\n",
      "iteration 400 / 2000: loss 5.921773\n",
      "iteration 500 / 2000: loss 5.069116\n",
      "iteration 600 / 2000: loss 4.111099\n",
      "iteration 700 / 2000: loss 3.530992\n",
      "iteration 800 / 2000: loss 3.034203\n",
      "iteration 900 / 2000: loss 2.791665\n",
      "iteration 1000 / 2000: loss 2.537608\n",
      "iteration 1100 / 2000: loss 2.252351\n",
      "iteration 1200 / 2000: loss 2.107588\n",
      "iteration 1300 / 2000: loss 2.088716\n",
      "iteration 1400 / 2000: loss 2.026464\n",
      "iteration 1500 / 2000: loss 1.848621\n",
      "iteration 1600 / 2000: loss 1.907593\n",
      "iteration 1700 / 2000: loss 1.877279\n",
      "iteration 1800 / 2000: loss 1.894802\n",
      "iteration 1900 / 2000: loss 1.758062\n",
      "iteration 0 / 2000: loss 25.537677\n",
      "iteration 100 / 2000: loss 15.128835\n",
      "iteration 200 / 2000: loss 10.466612\n",
      "iteration 300 / 2000: loss 7.241546\n",
      "iteration 400 / 2000: loss 5.258315\n",
      "iteration 500 / 2000: loss 4.103953\n",
      "iteration 600 / 2000: loss 3.200439\n",
      "iteration 700 / 2000: loss 2.648261\n",
      "iteration 800 / 2000: loss 2.251072\n",
      "iteration 900 / 2000: loss 2.105070\n",
      "iteration 1000 / 2000: loss 2.068350\n",
      "iteration 1100 / 2000: loss 2.093180\n",
      "iteration 1200 / 2000: loss 1.917690\n",
      "iteration 1300 / 2000: loss 1.838676\n",
      "iteration 1400 / 2000: loss 1.874075\n",
      "iteration 1500 / 2000: loss 1.899856\n",
      "iteration 1600 / 2000: loss 1.836888\n",
      "iteration 1700 / 2000: loss 1.908132\n",
      "iteration 1800 / 2000: loss 1.956370\n",
      "iteration 1900 / 2000: loss 1.831057\n",
      "iteration 0 / 2000: loss 31.965637\n",
      "iteration 100 / 2000: loss 17.210965\n",
      "iteration 200 / 2000: loss 10.498098\n",
      "iteration 300 / 2000: loss 6.431010\n",
      "iteration 400 / 2000: loss 4.374930\n",
      "iteration 500 / 2000: loss 3.232628\n",
      "iteration 600 / 2000: loss 2.690643\n",
      "iteration 700 / 2000: loss 2.189449\n",
      "iteration 800 / 2000: loss 2.021959\n",
      "iteration 900 / 2000: loss 1.928753\n",
      "iteration 1000 / 2000: loss 2.013053\n",
      "iteration 1100 / 2000: loss 1.919417\n",
      "iteration 1200 / 2000: loss 1.830006\n",
      "iteration 1300 / 2000: loss 1.887042\n",
      "iteration 1400 / 2000: loss 1.759641\n",
      "iteration 1500 / 2000: loss 1.831675\n",
      "iteration 1600 / 2000: loss 1.877180\n",
      "iteration 1700 / 2000: loss 1.766003\n",
      "iteration 1800 / 2000: loss 1.715980\n",
      "iteration 1900 / 2000: loss 1.840738\n",
      "iteration 0 / 2000: loss 39.981765\n",
      "iteration 100 / 2000: loss 18.250034\n",
      "iteration 200 / 2000: loss 9.722040\n",
      "iteration 300 / 2000: loss 5.476093\n",
      "iteration 400 / 2000: loss 3.538851\n",
      "iteration 500 / 2000: loss 2.635774\n",
      "iteration 600 / 2000: loss 2.115014\n",
      "iteration 700 / 2000: loss 2.041418\n",
      "iteration 800 / 2000: loss 1.950767\n",
      "iteration 900 / 2000: loss 1.955508\n",
      "iteration 1000 / 2000: loss 1.838186\n",
      "iteration 1100 / 2000: loss 1.795369\n",
      "iteration 1200 / 2000: loss 1.864220\n",
      "iteration 1300 / 2000: loss 1.840118\n",
      "iteration 1400 / 2000: loss 1.867978\n",
      "iteration 1500 / 2000: loss 1.783633\n",
      "iteration 1600 / 2000: loss 1.927890\n",
      "iteration 1700 / 2000: loss 1.881780\n",
      "iteration 1800 / 2000: loss 1.862254\n",
      "iteration 1900 / 2000: loss 1.822874\n",
      "iteration 0 / 2000: loss 46.696801\n",
      "iteration 100 / 2000: loss 19.140658\n",
      "iteration 200 / 2000: loss 8.924760\n",
      "iteration 300 / 2000: loss 4.736708\n",
      "iteration 400 / 2000: loss 3.052247\n",
      "iteration 500 / 2000: loss 2.407037\n",
      "iteration 600 / 2000: loss 2.017959\n",
      "iteration 700 / 2000: loss 2.059775\n",
      "iteration 800 / 2000: loss 1.794280\n",
      "iteration 900 / 2000: loss 1.908221\n",
      "iteration 1000 / 2000: loss 1.895642\n",
      "iteration 1100 / 2000: loss 1.875405\n",
      "iteration 1200 / 2000: loss 1.749038\n",
      "iteration 1300 / 2000: loss 1.764929\n",
      "iteration 1400 / 2000: loss 1.718380\n",
      "iteration 1500 / 2000: loss 1.866442\n",
      "iteration 1600 / 2000: loss 1.823600\n",
      "iteration 1700 / 2000: loss 1.835390\n",
      "iteration 1800 / 2000: loss 1.861614\n",
      "iteration 1900 / 2000: loss 1.930073\n",
      "iteration 0 / 2000: loss 54.136012\n",
      "iteration 100 / 2000: loss 19.222719\n",
      "iteration 200 / 2000: loss 8.057027\n",
      "iteration 300 / 2000: loss 4.031320\n",
      "iteration 400 / 2000: loss 2.685053\n",
      "iteration 500 / 2000: loss 2.180506\n",
      "iteration 600 / 2000: loss 1.949940\n",
      "iteration 700 / 2000: loss 1.918178\n",
      "iteration 800 / 2000: loss 1.839590\n",
      "iteration 900 / 2000: loss 1.876868\n",
      "iteration 1000 / 2000: loss 1.849633\n",
      "iteration 1100 / 2000: loss 1.970049\n",
      "iteration 1200 / 2000: loss 1.819339\n",
      "iteration 1300 / 2000: loss 1.950506\n",
      "iteration 1400 / 2000: loss 1.819725\n",
      "iteration 1500 / 2000: loss 1.923128\n",
      "iteration 1600 / 2000: loss 1.913207\n",
      "iteration 1700 / 2000: loss 1.904062\n",
      "iteration 1800 / 2000: loss 1.875021\n",
      "iteration 1900 / 2000: loss 1.895628\n",
      "iteration 0 / 2000: loss 61.503793\n",
      "iteration 100 / 2000: loss 19.033121\n",
      "iteration 200 / 2000: loss 7.247863\n",
      "iteration 300 / 2000: loss 3.508646\n",
      "iteration 400 / 2000: loss 2.395396\n",
      "iteration 500 / 2000: loss 1.950662\n",
      "iteration 600 / 2000: loss 1.894164\n",
      "iteration 700 / 2000: loss 1.889240\n",
      "iteration 800 / 2000: loss 1.891870\n",
      "iteration 900 / 2000: loss 1.932541\n",
      "iteration 1000 / 2000: loss 1.944146\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 1100 / 2000: loss 1.937178\n",
      "iteration 1200 / 2000: loss 1.863890\n",
      "iteration 1300 / 2000: loss 1.831781\n",
      "iteration 1400 / 2000: loss 1.889662\n",
      "iteration 1500 / 2000: loss 1.922603\n",
      "iteration 1600 / 2000: loss 2.047688\n",
      "iteration 1700 / 2000: loss 1.884077\n",
      "iteration 1800 / 2000: loss 1.935602\n",
      "iteration 1900 / 2000: loss 1.913168\n",
      "iteration 0 / 2000: loss 67.125764\n",
      "iteration 100 / 2000: loss 18.571206\n",
      "iteration 200 / 2000: loss 6.273172\n",
      "iteration 300 / 2000: loss 3.026541\n",
      "iteration 400 / 2000: loss 2.208510\n",
      "iteration 500 / 2000: loss 1.895907\n",
      "iteration 600 / 2000: loss 1.973884\n",
      "iteration 700 / 2000: loss 1.844341\n",
      "iteration 800 / 2000: loss 1.820220\n",
      "iteration 900 / 2000: loss 1.902422\n",
      "iteration 1000 / 2000: loss 2.011127\n",
      "iteration 1100 / 2000: loss 1.896117\n",
      "iteration 1200 / 2000: loss 1.812412\n",
      "iteration 1300 / 2000: loss 1.885839\n",
      "iteration 1400 / 2000: loss 1.940677\n",
      "iteration 1500 / 2000: loss 1.855069\n",
      "iteration 1600 / 2000: loss 1.970674\n",
      "iteration 1700 / 2000: loss 1.939971\n",
      "iteration 1800 / 2000: loss 1.868015\n",
      "iteration 1900 / 2000: loss 1.794236\n",
      "iteration 0 / 2000: loss 6.220268\n",
      "iteration 100 / 2000: loss 2.595376\n",
      "iteration 200 / 2000: loss 2.490970\n",
      "iteration 300 / 2000: loss 2.404364\n",
      "iteration 400 / 2000: loss 2.248657\n",
      "iteration 500 / 2000: loss 2.193602\n",
      "iteration 600 / 2000: loss 2.286582\n",
      "iteration 700 / 2000: loss 2.313159\n",
      "iteration 800 / 2000: loss 2.017405\n",
      "iteration 900 / 2000: loss 2.155013\n",
      "iteration 1000 / 2000: loss 2.190595\n",
      "iteration 1100 / 2000: loss 2.059189\n",
      "iteration 1200 / 2000: loss 2.283184\n",
      "iteration 1300 / 2000: loss 2.097703\n",
      "iteration 1400 / 2000: loss 2.082685\n",
      "iteration 1500 / 2000: loss 2.265648\n",
      "iteration 1600 / 2000: loss 2.118618\n",
      "iteration 1700 / 2000: loss 2.096268\n",
      "iteration 1800 / 2000: loss 2.202119\n",
      "iteration 1900 / 2000: loss 2.163596\n",
      "iteration 0 / 2000: loss 11.729067\n",
      "iteration 100 / 2000: loss 8.256449\n",
      "iteration 200 / 2000: loss 6.963395\n",
      "iteration 300 / 2000: loss 6.157490\n",
      "iteration 400 / 2000: loss 5.295843\n",
      "iteration 500 / 2000: loss 4.648391\n",
      "iteration 600 / 2000: loss 4.253006\n",
      "iteration 700 / 2000: loss 3.785945\n",
      "iteration 800 / 2000: loss 3.454458\n",
      "iteration 900 / 2000: loss 3.212201\n",
      "iteration 1000 / 2000: loss 3.000201\n",
      "iteration 1100 / 2000: loss 2.677353\n",
      "iteration 1200 / 2000: loss 2.631111\n",
      "iteration 1300 / 2000: loss 2.431148\n",
      "iteration 1400 / 2000: loss 2.332616\n",
      "iteration 1500 / 2000: loss 2.195106\n",
      "iteration 1600 / 2000: loss 2.180174\n",
      "iteration 1700 / 2000: loss 2.066472\n",
      "iteration 1800 / 2000: loss 2.113744\n",
      "iteration 1900 / 2000: loss 2.112447\n",
      "iteration 0 / 2000: loss 19.346427\n",
      "iteration 100 / 2000: loss 12.346819\n",
      "iteration 200 / 2000: loss 9.204479\n",
      "iteration 300 / 2000: loss 7.127458\n",
      "iteration 400 / 2000: loss 5.513876\n",
      "iteration 500 / 2000: loss 4.385960\n",
      "iteration 600 / 2000: loss 3.745727\n",
      "iteration 700 / 2000: loss 3.110851\n",
      "iteration 800 / 2000: loss 2.726288\n",
      "iteration 900 / 2000: loss 2.522318\n",
      "iteration 1000 / 2000: loss 2.148956\n",
      "iteration 1100 / 2000: loss 2.122452\n",
      "iteration 1200 / 2000: loss 1.963707\n",
      "iteration 1300 / 2000: loss 2.001817\n",
      "iteration 1400 / 2000: loss 1.912704\n",
      "iteration 1500 / 2000: loss 1.910433\n",
      "iteration 1600 / 2000: loss 1.781289\n",
      "iteration 1700 / 2000: loss 1.829174\n",
      "iteration 1800 / 2000: loss 1.890476\n",
      "iteration 1900 / 2000: loss 1.855057\n",
      "iteration 0 / 2000: loss 27.744612\n",
      "iteration 100 / 2000: loss 14.534596\n",
      "iteration 200 / 2000: loss 9.465353\n",
      "iteration 300 / 2000: loss 6.583829\n",
      "iteration 400 / 2000: loss 4.646339\n",
      "iteration 500 / 2000: loss 3.430822\n",
      "iteration 600 / 2000: loss 2.845456\n",
      "iteration 700 / 2000: loss 2.480842\n",
      "iteration 800 / 2000: loss 2.273773\n",
      "iteration 900 / 2000: loss 2.078809\n",
      "iteration 1000 / 2000: loss 2.003105\n",
      "iteration 1100 / 2000: loss 1.986672\n",
      "iteration 1200 / 2000: loss 1.876591\n",
      "iteration 1300 / 2000: loss 1.920245\n",
      "iteration 1400 / 2000: loss 1.796532\n",
      "iteration 1500 / 2000: loss 1.839789\n",
      "iteration 1600 / 2000: loss 1.833896\n",
      "iteration 1700 / 2000: loss 1.838557\n",
      "iteration 1800 / 2000: loss 1.682461\n",
      "iteration 1900 / 2000: loss 1.848693\n",
      "iteration 0 / 2000: loss 34.676444\n",
      "iteration 100 / 2000: loss 16.337457\n",
      "iteration 200 / 2000: loss 9.296834\n",
      "iteration 300 / 2000: loss 5.524048\n",
      "iteration 400 / 2000: loss 3.785358\n",
      "iteration 500 / 2000: loss 2.854677\n",
      "iteration 600 / 2000: loss 2.389754\n",
      "iteration 700 / 2000: loss 2.089002\n",
      "iteration 800 / 2000: loss 1.998969\n",
      "iteration 900 / 2000: loss 1.926821\n",
      "iteration 1000 / 2000: loss 1.768836\n",
      "iteration 1100 / 2000: loss 1.868445\n",
      "iteration 1200 / 2000: loss 1.908144\n",
      "iteration 1300 / 2000: loss 1.818264\n",
      "iteration 1400 / 2000: loss 1.852492\n",
      "iteration 1500 / 2000: loss 1.846507\n",
      "iteration 1600 / 2000: loss 1.830419\n",
      "iteration 1700 / 2000: loss 1.896387\n",
      "iteration 1800 / 2000: loss 1.739755\n",
      "iteration 1900 / 2000: loss 1.755767\n",
      "iteration 0 / 2000: loss 40.392490\n",
      "iteration 100 / 2000: loss 17.261977\n",
      "iteration 200 / 2000: loss 8.492374\n",
      "iteration 300 / 2000: loss 4.747207\n",
      "iteration 400 / 2000: loss 3.229176\n",
      "iteration 500 / 2000: loss 2.407140\n",
      "iteration 600 / 2000: loss 2.167891\n",
      "iteration 700 / 2000: loss 1.787179\n",
      "iteration 800 / 2000: loss 1.919082\n",
      "iteration 900 / 2000: loss 2.041411\n",
      "iteration 1000 / 2000: loss 1.914349\n",
      "iteration 1100 / 2000: loss 1.909380\n",
      "iteration 1200 / 2000: loss 1.818179\n",
      "iteration 1300 / 2000: loss 1.769077\n",
      "iteration 1400 / 2000: loss 1.861111\n",
      "iteration 1500 / 2000: loss 1.800113\n",
      "iteration 1600 / 2000: loss 1.794239\n",
      "iteration 1700 / 2000: loss 1.693540\n",
      "iteration 1800 / 2000: loss 1.869877\n",
      "iteration 1900 / 2000: loss 1.839465\n",
      "iteration 0 / 2000: loss 47.167428\n",
      "iteration 100 / 2000: loss 17.208641\n",
      "iteration 200 / 2000: loss 7.622906\n",
      "iteration 300 / 2000: loss 4.122312\n",
      "iteration 400 / 2000: loss 2.730810\n",
      "iteration 500 / 2000: loss 2.154202\n",
      "iteration 600 / 2000: loss 1.817764\n",
      "iteration 700 / 2000: loss 1.783810\n",
      "iteration 800 / 2000: loss 1.914461\n",
      "iteration 900 / 2000: loss 1.870402\n",
      "iteration 1000 / 2000: loss 1.890854\n",
      "iteration 1100 / 2000: loss 1.794398\n",
      "iteration 1200 / 2000: loss 1.861091\n",
      "iteration 1300 / 2000: loss 1.837501\n",
      "iteration 1400 / 2000: loss 1.811977\n",
      "iteration 1500 / 2000: loss 1.843313\n",
      "iteration 1600 / 2000: loss 1.877860\n",
      "iteration 1700 / 2000: loss 1.810367\n",
      "iteration 1800 / 2000: loss 1.868368\n",
      "iteration 1900 / 2000: loss 1.920398\n",
      "iteration 0 / 2000: loss 53.370150\n",
      "iteration 100 / 2000: loss 16.987193\n",
      "iteration 200 / 2000: loss 6.644055\n",
      "iteration 300 / 2000: loss 3.484136\n",
      "iteration 400 / 2000: loss 2.318267\n",
      "iteration 500 / 2000: loss 2.120467\n",
      "iteration 600 / 2000: loss 1.934066\n",
      "iteration 700 / 2000: loss 1.893903\n",
      "iteration 800 / 2000: loss 1.871455\n",
      "iteration 900 / 2000: loss 1.833631\n",
      "iteration 1000 / 2000: loss 1.924765\n",
      "iteration 1100 / 2000: loss 1.738416\n",
      "iteration 1200 / 2000: loss 1.881295\n",
      "iteration 1300 / 2000: loss 1.885859\n",
      "iteration 1400 / 2000: loss 1.901512\n",
      "iteration 1500 / 2000: loss 1.961359\n",
      "iteration 1600 / 2000: loss 1.869934\n",
      "iteration 1700 / 2000: loss 1.818486\n",
      "iteration 1800 / 2000: loss 1.868289\n",
      "iteration 1900 / 2000: loss 1.847167\n",
      "iteration 0 / 2000: loss 59.195077\n",
      "iteration 100 / 2000: loss 16.668538\n",
      "iteration 200 / 2000: loss 5.771170\n",
      "iteration 300 / 2000: loss 2.957262\n",
      "iteration 400 / 2000: loss 2.099259\n",
      "iteration 500 / 2000: loss 1.922550\n",
      "iteration 600 / 2000: loss 1.965449\n",
      "iteration 700 / 2000: loss 1.861187\n",
      "iteration 800 / 2000: loss 1.923194\n",
      "iteration 900 / 2000: loss 1.926352\n",
      "iteration 1000 / 2000: loss 1.792120\n",
      "iteration 1100 / 2000: loss 1.862077\n",
      "iteration 1200 / 2000: loss 1.876720\n",
      "iteration 1300 / 2000: loss 1.825504\n",
      "iteration 1400 / 2000: loss 1.926473\n",
      "iteration 1500 / 2000: loss 1.903213\n",
      "iteration 1600 / 2000: loss 1.840021\n",
      "iteration 1700 / 2000: loss 1.939381\n",
      "iteration 1800 / 2000: loss 1.986760\n",
      "iteration 1900 / 2000: loss 1.911267\n",
      "iteration 0 / 2000: loss 65.736269\n",
      "iteration 100 / 2000: loss 15.902358\n",
      "iteration 200 / 2000: loss 5.119211\n",
      "iteration 300 / 2000: loss 2.518785\n",
      "iteration 400 / 2000: loss 2.027598\n",
      "iteration 500 / 2000: loss 1.985002\n",
      "iteration 600 / 2000: loss 1.911333\n",
      "iteration 700 / 2000: loss 1.859483\n",
      "iteration 800 / 2000: loss 1.939857\n",
      "iteration 900 / 2000: loss 1.798151\n",
      "iteration 1000 / 2000: loss 1.889309\n",
      "iteration 1100 / 2000: loss 2.007570\n",
      "iteration 1200 / 2000: loss 1.818331\n",
      "iteration 1300 / 2000: loss 1.933645\n",
      "iteration 1400 / 2000: loss 1.828719\n",
      "iteration 1500 / 2000: loss 1.781818\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 1600 / 2000: loss 1.915913\n",
      "iteration 1700 / 2000: loss 1.934217\n",
      "iteration 1800 / 2000: loss 1.863821\n",
      "iteration 1900 / 2000: loss 1.821264\n",
      "iteration 0 / 2000: loss 5.685438\n",
      "iteration 100 / 2000: loss 2.795097\n",
      "iteration 200 / 2000: loss 2.666134\n",
      "iteration 300 / 2000: loss 2.600846\n",
      "iteration 400 / 2000: loss 2.294817\n",
      "iteration 500 / 2000: loss 2.278886\n",
      "iteration 600 / 2000: loss 2.239203\n",
      "iteration 700 / 2000: loss 2.387878\n",
      "iteration 800 / 2000: loss 2.160814\n",
      "iteration 900 / 2000: loss 2.390038\n",
      "iteration 1000 / 2000: loss 2.036119\n",
      "iteration 1100 / 2000: loss 2.177142\n",
      "iteration 1200 / 2000: loss 2.088688\n",
      "iteration 1300 / 2000: loss 2.040979\n",
      "iteration 1400 / 2000: loss 2.026663\n",
      "iteration 1500 / 2000: loss 1.925593\n",
      "iteration 1600 / 2000: loss 2.068253\n",
      "iteration 1700 / 2000: loss 1.984947\n",
      "iteration 1800 / 2000: loss 2.130893\n",
      "iteration 1900 / 2000: loss 2.202270\n",
      "iteration 0 / 2000: loss 12.716220\n",
      "iteration 100 / 2000: loss 8.192326\n",
      "iteration 200 / 2000: loss 6.879818\n",
      "iteration 300 / 2000: loss 5.925728\n",
      "iteration 400 / 2000: loss 5.098776\n",
      "iteration 500 / 2000: loss 4.564423\n",
      "iteration 600 / 2000: loss 4.151455\n",
      "iteration 700 / 2000: loss 3.761820\n",
      "iteration 800 / 2000: loss 3.348925\n",
      "iteration 900 / 2000: loss 3.128854\n",
      "iteration 1000 / 2000: loss 2.828072\n",
      "iteration 1100 / 2000: loss 2.504098\n",
      "iteration 1200 / 2000: loss 2.524814\n",
      "iteration 1300 / 2000: loss 2.340350\n",
      "iteration 1400 / 2000: loss 2.247309\n",
      "iteration 1500 / 2000: loss 2.206695\n",
      "iteration 1600 / 2000: loss 2.066685\n",
      "iteration 1700 / 2000: loss 2.027181\n",
      "iteration 1800 / 2000: loss 2.125349\n",
      "iteration 1900 / 2000: loss 1.950962\n",
      "iteration 0 / 2000: loss 18.529195\n",
      "iteration 100 / 2000: loss 11.826307\n",
      "iteration 200 / 2000: loss 8.468277\n",
      "iteration 300 / 2000: loss 6.503556\n",
      "iteration 400 / 2000: loss 4.951893\n",
      "iteration 500 / 2000: loss 3.951385\n",
      "iteration 600 / 2000: loss 3.334152\n",
      "iteration 700 / 2000: loss 2.920263\n",
      "iteration 800 / 2000: loss 2.546331\n",
      "iteration 900 / 2000: loss 2.237375\n",
      "iteration 1000 / 2000: loss 2.230590\n",
      "iteration 1100 / 2000: loss 2.094027\n",
      "iteration 1200 / 2000: loss 2.080858\n",
      "iteration 1300 / 2000: loss 1.823244\n",
      "iteration 1400 / 2000: loss 1.942054\n",
      "iteration 1500 / 2000: loss 1.841991\n",
      "iteration 1600 / 2000: loss 1.743045\n",
      "iteration 1700 / 2000: loss 1.999444\n",
      "iteration 1800 / 2000: loss 1.730814\n",
      "iteration 1900 / 2000: loss 1.825381\n",
      "iteration 0 / 2000: loss 25.769637\n",
      "iteration 100 / 2000: loss 14.047749\n",
      "iteration 200 / 2000: loss 8.798893\n",
      "iteration 300 / 2000: loss 5.820409\n",
      "iteration 400 / 2000: loss 4.060115\n",
      "iteration 500 / 2000: loss 3.196600\n",
      "iteration 600 / 2000: loss 2.564596\n",
      "iteration 700 / 2000: loss 2.189909\n",
      "iteration 800 / 2000: loss 2.146219\n",
      "iteration 900 / 2000: loss 1.970131\n",
      "iteration 1000 / 2000: loss 1.889561\n",
      "iteration 1100 / 2000: loss 1.885017\n",
      "iteration 1200 / 2000: loss 1.785448\n",
      "iteration 1300 / 2000: loss 1.855330\n",
      "iteration 1400 / 2000: loss 1.716387\n",
      "iteration 1500 / 2000: loss 1.850458\n",
      "iteration 1600 / 2000: loss 1.719180\n",
      "iteration 1700 / 2000: loss 1.841698\n",
      "iteration 1800 / 2000: loss 1.780180\n",
      "iteration 1900 / 2000: loss 1.871468\n",
      "iteration 0 / 2000: loss 33.366729\n",
      "iteration 100 / 2000: loss 15.128088\n",
      "iteration 200 / 2000: loss 8.401082\n",
      "iteration 300 / 2000: loss 4.936958\n",
      "iteration 400 / 2000: loss 3.223608\n",
      "iteration 500 / 2000: loss 2.539230\n",
      "iteration 600 / 2000: loss 2.222273\n",
      "iteration 700 / 2000: loss 2.002498\n",
      "iteration 800 / 2000: loss 1.937195\n",
      "iteration 900 / 2000: loss 1.981605\n",
      "iteration 1000 / 2000: loss 1.858139\n",
      "iteration 1100 / 2000: loss 1.733227\n",
      "iteration 1200 / 2000: loss 1.894915\n",
      "iteration 1300 / 2000: loss 1.848783\n",
      "iteration 1400 / 2000: loss 1.829161\n",
      "iteration 1500 / 2000: loss 1.895348\n",
      "iteration 1600 / 2000: loss 1.938552\n",
      "iteration 1700 / 2000: loss 1.801462\n",
      "iteration 1800 / 2000: loss 1.928167\n",
      "iteration 1900 / 2000: loss 1.839207\n",
      "iteration 0 / 2000: loss 40.667804\n",
      "iteration 100 / 2000: loss 15.864703\n",
      "iteration 200 / 2000: loss 7.443962\n",
      "iteration 300 / 2000: loss 4.148021\n",
      "iteration 400 / 2000: loss 2.758359\n",
      "iteration 500 / 2000: loss 2.167163\n",
      "iteration 600 / 2000: loss 2.079618\n",
      "iteration 700 / 2000: loss 1.889306\n",
      "iteration 800 / 2000: loss 1.798730\n",
      "iteration 900 / 2000: loss 1.970599\n",
      "iteration 1000 / 2000: loss 1.888658\n",
      "iteration 1100 / 2000: loss 1.848353\n",
      "iteration 1200 / 2000: loss 1.835112\n",
      "iteration 1300 / 2000: loss 1.868064\n",
      "iteration 1400 / 2000: loss 1.888170\n",
      "iteration 1500 / 2000: loss 1.890264\n",
      "iteration 1600 / 2000: loss 1.928076\n",
      "iteration 1700 / 2000: loss 1.855511\n",
      "iteration 1800 / 2000: loss 1.878054\n",
      "iteration 1900 / 2000: loss 1.854085\n",
      "iteration 0 / 2000: loss 46.908800\n",
      "iteration 100 / 2000: loss 15.695697\n",
      "iteration 200 / 2000: loss 6.498113\n",
      "iteration 300 / 2000: loss 3.445922\n",
      "iteration 400 / 2000: loss 2.448513\n",
      "iteration 500 / 2000: loss 2.113604\n",
      "iteration 600 / 2000: loss 1.825126\n",
      "iteration 700 / 2000: loss 1.894554\n",
      "iteration 800 / 2000: loss 1.930553\n",
      "iteration 900 / 2000: loss 1.899377\n",
      "iteration 1000 / 2000: loss 1.832714\n",
      "iteration 1100 / 2000: loss 1.823828\n",
      "iteration 1200 / 2000: loss 1.791255\n",
      "iteration 1300 / 2000: loss 1.862967\n",
      "iteration 1400 / 2000: loss 1.845265\n",
      "iteration 1500 / 2000: loss 1.885855\n",
      "iteration 1600 / 2000: loss 1.886823\n",
      "iteration 1700 / 2000: loss 1.915665\n",
      "iteration 1800 / 2000: loss 1.976728\n",
      "iteration 1900 / 2000: loss 1.831245\n",
      "iteration 0 / 2000: loss 53.064415\n",
      "iteration 100 / 2000: loss 15.530671\n",
      "iteration 200 / 2000: loss 5.753734\n",
      "iteration 300 / 2000: loss 3.070436\n",
      "iteration 400 / 2000: loss 2.116239\n",
      "iteration 500 / 2000: loss 1.994506\n",
      "iteration 600 / 2000: loss 1.871449\n",
      "iteration 700 / 2000: loss 1.987982\n",
      "iteration 800 / 2000: loss 1.907764\n",
      "iteration 900 / 2000: loss 1.909008\n",
      "iteration 1000 / 2000: loss 1.885419\n",
      "iteration 1100 / 2000: loss 1.830555\n",
      "iteration 1200 / 2000: loss 1.732107\n",
      "iteration 1300 / 2000: loss 1.807611\n",
      "iteration 1400 / 2000: loss 1.955553\n",
      "iteration 1500 / 2000: loss 1.721676\n",
      "iteration 1600 / 2000: loss 1.922094\n",
      "iteration 1700 / 2000: loss 1.982200\n",
      "iteration 1800 / 2000: loss 1.810590\n",
      "iteration 1900 / 2000: loss 1.777130\n",
      "iteration 0 / 2000: loss 61.079421\n",
      "iteration 100 / 2000: loss 14.979042\n",
      "iteration 200 / 2000: loss 4.934212\n",
      "iteration 300 / 2000: loss 2.645139\n",
      "iteration 400 / 2000: loss 2.056745\n",
      "iteration 500 / 2000: loss 1.958633\n",
      "iteration 600 / 2000: loss 1.872189\n",
      "iteration 700 / 2000: loss 1.867460\n",
      "iteration 800 / 2000: loss 1.891831\n",
      "iteration 900 / 2000: loss 1.943511\n",
      "iteration 1000 / 2000: loss 1.818112\n",
      "iteration 1100 / 2000: loss 1.811909\n",
      "iteration 1200 / 2000: loss 1.912317\n",
      "iteration 1300 / 2000: loss 1.788487\n",
      "iteration 1400 / 2000: loss 1.908105\n",
      "iteration 1500 / 2000: loss 1.873219\n",
      "iteration 1600 / 2000: loss 1.869813\n",
      "iteration 1700 / 2000: loss 1.918561\n",
      "iteration 1800 / 2000: loss 1.954418\n",
      "iteration 1900 / 2000: loss 1.899823\n",
      "iteration 0 / 2000: loss 66.448725\n",
      "iteration 100 / 2000: loss 14.014351\n",
      "iteration 200 / 2000: loss 4.342541\n",
      "iteration 300 / 2000: loss 2.324955\n",
      "iteration 400 / 2000: loss 1.950377\n",
      "iteration 500 / 2000: loss 1.890747\n",
      "iteration 600 / 2000: loss 1.875454\n",
      "iteration 700 / 2000: loss 1.895759\n",
      "iteration 800 / 2000: loss 1.876978\n",
      "iteration 900 / 2000: loss 1.890716\n",
      "iteration 1000 / 2000: loss 1.911568\n",
      "iteration 1100 / 2000: loss 1.812854\n",
      "iteration 1200 / 2000: loss 1.891575\n",
      "iteration 1300 / 2000: loss 1.957691\n",
      "iteration 1400 / 2000: loss 1.891056\n",
      "iteration 1500 / 2000: loss 1.822906\n",
      "iteration 1600 / 2000: loss 1.991799\n",
      "iteration 1700 / 2000: loss 1.903294\n",
      "iteration 1800 / 2000: loss 1.830345\n",
      "iteration 1900 / 2000: loss 1.879907\n",
      "lr 3.000000e-07 reg 2.000000e+03 train accuracy: 0.390755 val accuracy: 0.391000\n",
      "lr 4.888889e-07 reg 2.000000e+03 train accuracy: 0.391531 val accuracy: 0.391000\n",
      "lr 6.777778e-07 reg 2.000000e+03 train accuracy: 0.393408 val accuracy: 0.394000\n",
      "lr 8.666667e-07 reg 2.000000e+03 train accuracy: 0.386959 val accuracy: 0.390000\n",
      "lr 1.055556e-06 reg 2.000000e+03 train accuracy: 0.386449 val accuracy: 0.398000\n",
      "lr 1.244444e-06 reg 2.000000e+03 train accuracy: 0.385347 val accuracy: 0.396000\n",
      "lr 1.433333e-06 reg 2.000000e+03 train accuracy: 0.388408 val accuracy: 0.394000\n",
      "lr 1.622222e-06 reg 2.000000e+03 train accuracy: 0.382510 val accuracy: 0.398000\n",
      "lr 1.811111e-06 reg 2.000000e+03 train accuracy: 0.376980 val accuracy: 0.381000\n",
      "lr 2.000000e-06 reg 2.000000e+03 train accuracy: 0.373061 val accuracy: 0.381000\n",
      "best validation accuracy achieved during cross-validation: 0.419000\n"
     ]
    }
   ],
   "source": [
    "# Use the validation set to tune hyperparameters (regularization strength and\n",
    "# learning rate). You should experiment with different ranges for the learning\n",
    "# rates and regularization strengths; if you are careful you should be able to\n",
    "# get a classification accuracy of over 0.35 on the validation set.\n",
    "\n",
    "from cs231n.classifiers import Softmax\n",
    "results = {}\n",
    "best_val = -1\n",
    "best_softmax = None\n",
    "\n",
    "################################################################################\n",
    "# TODO:                                                                        #\n",
    "# Use the validation set to set the learning rate and regularization strength. #\n",
    "# This should be identical to the validation that you did for the SVM; save    #\n",
    "# the best trained softmax classifer in best_softmax.                          #\n",
    "################################################################################\n",
    "\n",
    "# *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
    "##################################\n",
    "# rk's remark:                   #\n",
    "#          random search         #\n",
    "##################################\n",
    "\n",
    "# sample learning_rates\n",
    "exp_lr = -10 * np.random.rand(10)\n",
    "learning_rates = np.power(10, exp_lr)\n",
    "\n",
    "# sample regs\n",
    "exp_reg = 7 * np.random.rand(10)\n",
    "reg_strengths = np.power(10, exp_reg)\n",
    "para_group = [(np.random.choice(learning_rates, replace=False), \n",
    "               np.random.choice(reg_strengths, replace=False)) for _ in range(20)]\n",
    "\n",
    "###  comment out after try out coarse grid\n",
    "# for lr, reg in para_group:\n",
    "#     softmax = Softmax()\n",
    "#     loss_history = softmax.train(X_train, y_train, learning_rate=lr, reg=reg,\n",
    "#                                  num_iters=2000, verbose=True)\n",
    "#     y_train_pred = softmax.predict(X_train)\n",
    "#     train_acc = np.sum((y_train_pred == y_train)) / y_train.shape[0]\n",
    "#     y_val_pred = softmax.predict(X_val)\n",
    "#     val_acc = np.sum((y_val_pred == y_val)) / y_val.shape[0]\n",
    "#     if(val_acc > best_val):\n",
    "#         best_val=val_acc\n",
    "#         best_softmax=softmax\n",
    "#     results[(lr,reg)]=(train_acc,val_acc)\n",
    "###  best result from coarse grid: \n",
    "###  => best validation accuracy achieved during cross-validation: 0.416000\n",
    "\n",
    "### refine grid\n",
    "lr_fine = [3e-7, 2e-6]\n",
    "reg_fine = [1e1, 2e3]\n",
    "\n",
    "for lr in np.linspace(lr_fine[0], lr_fine[1], 10):\n",
    "    for reg in np.linspace(reg_fine[0], reg_fine[1], 10):\n",
    "        softmax = Softmax()\n",
    "        loss_history = softmax.train(X_train, y_train, learning_rate=lr, reg=reg,\n",
    "                                     num_iters=2000, verbose=True)\n",
    "        y_train_pred = softmax.predict(X_train)\n",
    "        train_acc = np.sum((y_train_pred == y_train)) / y_train.shape[0]\n",
    "        y_val_pred = softmax.predict(X_val)\n",
    "        val_acc = np.sum((y_val_pred == y_val)) / y_val.shape[0]\n",
    "        if(val_acc > best_val):\n",
    "            best_val=val_acc\n",
    "            best_softmax=softmax\n",
    "    results[(lr,reg)]=(train_acc,val_acc)\n",
    "# *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
    "    \n",
    "# Print out results.\n",
    "for lr, reg in sorted(results):\n",
    "    train_accuracy, val_accuracy = results[(lr, reg)]\n",
    "    print('lr %e reg %e train accuracy: %f val accuracy: %f' % (\n",
    "                lr, reg, train_accuracy, val_accuracy))\n",
    "    \n",
    "print('best validation accuracy achieved during cross-validation: %f' % best_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8a1f9db3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-03T05:27:26.407229Z",
     "start_time": "2023-10-03T05:27:26.375943Z"
    },
    "test": "test"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "softmax on raw pixels final test set accuracy: 0.380000\n"
     ]
    }
   ],
   "source": [
    "# evaluate on test set\n",
    "# Evaluate the best softmax on test set\n",
    "y_test_pred = best_softmax.predict(X_test)\n",
    "test_accuracy = np.mean(y_test == y_test_pred)\n",
    "print('softmax on raw pixels final test set accuracy: %f' % (test_accuracy, ))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86b9b65c",
   "metadata": {
    "tags": [
     "pdf-inline"
    ]
   },
   "source": [
    "**Inline Question 2** - *True or False*\n",
    "\n",
    "Suppose the overall training loss is defined as the sum of the per-datapoint loss over all training examples. It is possible to add a new datapoint to a training set that would leave the SVM loss unchanged, but this is not the case with the Softmax classifier loss.\n",
    "\n",
    "$\\color{blue}{\\textit Your Answer:}$\n",
    "\n",
    "\n",
    "$\\color{blue}{\\textit Your Explanation:}$\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "009f08b9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-03T05:27:26.811002Z",
     "start_time": "2023-10-03T05:27:26.408990Z"
    }
   },
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x600 with 10 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Visualize the learned weights for each class\n",
    "w = best_softmax.W[:-1,:] # strip out the bias\n",
    "w = w.reshape(32, 32, 3, 10)\n",
    "\n",
    "w_min, w_max = np.min(w), np.max(w)\n",
    "\n",
    "classes = ['plane', 'car', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck']\n",
    "for i in range(10):\n",
    "    plt.subplot(2, 5, i + 1)\n",
    "    \n",
    "    # Rescale the weights to be between 0 and 255\n",
    "    wimg = 255.0 * (w[:, :, :, i].squeeze() - w_min) / (w_max - w_min)\n",
    "    plt.imshow(wimg.astype('uint8'))\n",
    "    plt.axis('off')\n",
    "    plt.title(classes[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3569b36b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
